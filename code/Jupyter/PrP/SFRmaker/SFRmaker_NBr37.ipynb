{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b3584c1",
   "metadata": {},
   "source": [
    "# 0. Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702e13cb",
   "metadata": {},
   "source": [
    "## 0.0. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6599c8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import primod\n",
    "import imod\n",
    "from imod import msw\n",
    "from imod import mf6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b06185a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imod: 1.0.0rc4\n",
      "primod path: c:\\Users\\Karam014\\OneDrive - Universiteit Utrecht\\WS_Mdl\\code\\.pixi\\envs\\default\\Lib\\site-packages\\primod\\__init__.py\n",
      "primod commit: 6616b3300178c787685660ca2e1130b6d8079a63\n",
      "has get_diskey: False\n"
     ]
    }
   ],
   "source": [
    "import importlib.metadata as md, json, pathlib, imod, primod\n",
    "from imod.mf6 import GroundwaterFlowModel as GWF\n",
    "\n",
    "print(\"imod:\", md.version(\"imod\"))\n",
    "print(\"primod path:\", primod.__file__)\n",
    "du = next(p for p in md.distribution(\"primod\").files if p.name==\"direct_url.json\")\n",
    "print(\"primod commit:\", json.loads(pathlib.Path(md.distribution(\"primod\").locate_file(du)).read_text())[\"vcs_info\"][\"commit_id\"])\n",
    "print(\"has get_diskey:\", hasattr(GWF, \"get_diskey\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075c0e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib as IL\n",
    "IL.reload(primod)\n",
    "IL.reload(imod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81b0924",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import listdir as LD, makedirs as MDs\n",
    "from os.path import join as PJ, basename as PBN, dirname as PDN, exists as PE\n",
    "import shutil as sh\n",
    "import pandas as pd\n",
    "from datetime import datetime as DT\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4d2d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import WS_Mdl.utils as U\n",
    "import WS_Mdl.utils_imod as UIM\n",
    "import WS_Mdl.calcs as C\n",
    "import WS_Mdl.geo as G\n",
    "from WS_Mdl.utils import bold, style_reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23de4e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib as IL\n",
    "IL.reload(U)\n",
    "IL.reload(UIM)\n",
    "IL.reload(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f10ee69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import sfrmaker and other necessary packages for SFR network creation\n",
    "import sfrmaker as sfr\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from shapely.geometry import MultiLineString, box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca6e21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itables import init_notebook_mode\n",
    "init_notebook_mode(all_interactive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85a3a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import primod\n",
    "import imod\n",
    "from imod import msw\n",
    "from imod import mf6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ce458b",
   "metadata": {},
   "outputs": [],
   "source": [
    "IL.reload(primod)\n",
    "IL.reload(imod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e229c42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import DRN_SFR_match\n",
    "from DRN_SFR_match import match_cells_to_SFR\n",
    "import w_MVR\n",
    "from w_MVR import w_MVR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b789b34",
   "metadata": {},
   "source": [
    "## 0.1. Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b07653",
   "metadata": {},
   "outputs": [],
   "source": [
    "Pa_GPkg_1ry = r'C:\\OD\\WS_Mdl\\models\\NBr\\PrP\\SFR\\BrabantseDelta\\WBD_SW_NW_1ry.gpkg' \n",
    "Pa_GPkg = r\"C:\\OD\\WS_Mdl\\models\\NBr\\PrP\\SFR\\BrabantseDelta\\acceptatiedatabase.gdb\"\n",
    "detailed = 'hydroobject'\n",
    "primary = 'LEGGER_VASTGESTELD_WATERLOOP_CATEGORIE_A'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2201f84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "MdlN = 'NBr37'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da68110",
   "metadata": {},
   "outputs": [],
   "source": [
    "U.set_verbose(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8464b2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load paths and variables from PRJ & INI\n",
    "d_Pa = U.get_MdlN_Pa(MdlN)\n",
    "Pa_PRJ = d_Pa['PRJ']\n",
    "Dir_PRJ = PDN(Pa_PRJ)\n",
    "d_INI = U.INI_to_d(d_Pa['INI'])\n",
    "Xmin, Ymin, Xmax, Ymax, cellsize, N_R, N_C = U.Mdl_Dmns_from_INI(d_Pa['INI'])\n",
    "SP_date_1st, SP_date_last = [DT.strftime(DT.strptime(d_INI[f'{i}'], '%Y%m%d'), '%Y-%m-%d') for i in ['SDATE', 'EDATE']]\n",
    "dx = dy = float(d_INI['CELLSIZE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314b442a",
   "metadata": {},
   "outputs": [],
   "source": [
    "U.Mdl_Dmns_from_INI(d_Pa['INI'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b4a7f7",
   "metadata": {},
   "source": [
    "# 1. Load Model Ins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b323952",
   "metadata": {},
   "source": [
    "## 1.0. Load PRJ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2fc3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "PRJ_, PRJ_OBS = UIM.o_PRJ_with_OBS(Pa_PRJ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0960a113",
   "metadata": {},
   "outputs": [],
   "source": [
    "PRJ, period_data = PRJ_[0], PRJ_[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cfe111",
   "metadata": {},
   "source": [
    "## 1.1. Load DIS and limit to Mdl Aa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347d651b",
   "metadata": {},
   "outputs": [],
   "source": [
    "PRJ_regrid = UIM.regrid_PRJ(PRJ, MdlN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02039dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "BND = PRJ_regrid['bnd']['ibound']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068608ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set outer boundaries to -1 (for CHD)\n",
    "\n",
    "# Get the coordinate indices for boundaries\n",
    "y_coords = BND.y\n",
    "x_coords = BND.x\n",
    "first_y = y_coords.isel(y=0)  # First y coordinate\n",
    "last_y = y_coords.isel(y=-1)  # Last y coordinate  \n",
    "first_x = x_coords.isel(x=0)  # First x coordinate\n",
    "last_x = x_coords.isel(x=-1)  # Last x coordinate\n",
    "\n",
    "# Set boundary values using .loc indexing\n",
    "BND.loc[:, first_y, :] = -1  # Top row (all layers, first y, all x)\n",
    "BND.loc[:, last_y, :] = -1   # Bottom row (all layers, last y, all x)\n",
    "BND.loc[:, :, first_x] = -1  # Left column (all layers, all y, first x)  \n",
    "BND.loc[:, :, last_x] = -1   # Right column (all layers, all y, last x)\n",
    "\n",
    "print(\"âœ… Boundary conditions set successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31569d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "BND.isel(layer=0, x=range(0,10), y=range(0,10)).plot.imshow(cmap='viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deed1ee2",
   "metadata": {},
   "source": [
    "## 1.2. Load MF6 Mdl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51582fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "times = pd.date_range(SP_date_1st, SP_date_last, freq='D')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48aef4a2",
   "metadata": {},
   "source": [
    "Using original PRJ to load MF6 Mdl gives warnings (and it's very slow). Thus, well use the regridded PRJ, which is much faster. It can be further sped up by multi-processing, but this is not implemented yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5357ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sim_MF6 = mf6.Modflow6Simulation.from_imod5_data(PRJ_regrid, period_data, times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744a96ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "MF6_Mdl = Sim_MF6['imported_model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875d5c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "MF6_Mdl[\"oc\"] = mf6.OutputControl(save_head=\"last\", save_budget=\"last\")\n",
    "Sim_MF6[\"ims\"] = UIM.mf6_solution_moderate_settings() # Mimic iMOD5's \"Moderate\" settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7b7fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "MF6_DIS = MF6_Mdl[\"dis\"]  # This gets the OLD 100m grid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402356eb",
   "metadata": {},
   "source": [
    "## 1.3. Load MSW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696a3097",
   "metadata": {},
   "source": [
    "### 1.3.0. Fix mete_grid.inp relative paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05c28be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the mete_grid.inp path in the PRJ_MSW_for_MSW dictionary\n",
    "PRJ['extra']['paths'][2][0] = UIM.mete_grid_Cvt_to_AbsPa(Pa_PRJ, PRJ_regrid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808649bd",
   "metadata": {},
   "source": [
    "### 1.3.2. Finally load MSW Sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9cc8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the MetaSwap model\n",
    "PRJ_MSW = {'cap': PRJ_regrid.copy()['cap'], 'extra': PRJ_regrid.copy()['extra']}\n",
    "MSW_Mdl = msw.MetaSwapModel.from_imod5_data(PRJ_MSW, MF6_DIS, times)\n",
    "print(\"ðŸŸ¢ - MetaSwap model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0555966b",
   "metadata": {},
   "source": [
    "## 1.4. Connect MF6 to MetaSWAP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a467f1",
   "metadata": {},
   "source": [
    "### 1.4.1. Clip models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd64d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sim_MF6_AoI = Sim_MF6.clip_box(x_min=Xmin, x_max=Xmax, y_min=Ymin, y_max=Ymax)\n",
    "MF6_Mdl_AoI = Sim_MF6_AoI['imported_model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d176a143",
   "metadata": {},
   "outputs": [],
   "source": [
    "MSW_Mdl_AoI = MSW_Mdl.clip_box(x_min=Xmin, x_max=Xmax, y_min=Ymin, y_max=Ymax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d4565c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"MF6 Model AoI DIS shape: {MF6_Mdl_AoI['dis'].dataset.sizes}\")\n",
    "print(f\"MSW Model AoI grid shape: {MSW_Mdl_AoI['grid'].dataset.sizes}\")\n",
    "print(\"âœ… Both models successfully clipped to Area of Interest with compatible discretization!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176414e0",
   "metadata": {},
   "source": [
    "## 1.5. Load & Cleanup models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ada8c7",
   "metadata": {},
   "source": [
    "### 1.5.0. Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c914df",
   "metadata": {},
   "outputs": [],
   "source": [
    "for pkg in MF6_Mdl_AoI.values():\n",
    "    pkg.dataset.load()\n",
    "\n",
    "for pkg in MSW_Mdl_AoI.values():\n",
    "    pkg.dataset.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2cb255",
   "metadata": {},
   "source": [
    "### 1.5.1. MF6 mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5d3b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mask from current regridded model (not the old one)\n",
    "mask = MF6_Mdl_AoI.domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71be718e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix CHD package layer ordering issue (layers must be monotonically increasing)\n",
    "from imod.mf6 import ConstantHead\n",
    "chd_pkg = Sim_MF6_AoI['imported_model']['chd_merged']\n",
    "head_data_sorted = chd_pkg.dataset['head'].load().sortby('layer')\n",
    "Sim_MF6_AoI['imported_model']['chd_merged'] = ConstantHead(head=head_data_sorted, validate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38983eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sim_MF6_AoI.mask_all_models(mask)\n",
    "DIS_AoI = MF6_Mdl_AoI[\"dis\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcc5538",
   "metadata": {},
   "source": [
    "### 1.5.2. Cleanup MF6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf51b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    for Pkg in [i for i in MF6_Mdl_AoI.keys() if ('riv' in i.lower()) or ('drn' in i.lower())]:\n",
    "        MF6_Mdl_AoI[Pkg].cleanup(DIS_AoI)\n",
    "except:\n",
    "    print('Failed to cleanup packages. Proceeding without cleanup. Fingers crossed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c98f3a",
   "metadata": {},
   "source": [
    "### 1.5.3 Cleanup MetaSWAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0cca65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "MSW_Mdl_AoI[\"grid\"].dataset[\"rootzone_depth\"] = MSW_Mdl_AoI[\"grid\"].dataset[\"rootzone_depth\"].fillna(1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39003970",
   "metadata": {},
   "source": [
    "## 1.6. Couple & Write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef22db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "metamod_coupling = primod.MetaModDriverCoupling(mf6_model=\"imported_model\", mf6_recharge_package=\"msw-rch\", mf6_wel_package=\"msw-sprinkling\")\n",
    "metamod = primod.MetaMod(MSW_Mdl_AoI, Sim_MF6_AoI, coupling_list=[metamod_coupling])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72642cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(d_Pa['Pa_MdlN'], exist_ok=True) # Create simulation directory if it doesn't exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d16ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use correct paths from d_Pa instead of hardcoded paths\n",
    "Pa_MF6_DLL = d_Pa['MF6_DLL']\n",
    "Pa_MSW_DLL = d_Pa['MSW_DLL']\n",
    "Pa_IMC = d_Pa['coupler_Exe']\n",
    "\n",
    "print(f\"âœ… MF6 DLL path: {Pa_MF6_DLL}\")\n",
    "print(f\"âœ… MSW DLL path: {Pa_MSW_DLL}\")\n",
    "print(f\"âœ… Coupler exe path: {d_Pa['coupler_Exe']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112896a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "metamod.write(directory=d_Pa['Pa_MdlN'], modflow6_dll=Pa_MF6_DLL, metaswap_dll=Pa_MSW_DLL, metaswap_dll_dependency=PDN(Pa_MF6_DLL))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ccba65",
   "metadata": {},
   "source": [
    "# 2. Create SFR lines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbfb348",
   "metadata": {},
   "source": [
    "## 2.1.Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854dc11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "GDF_1ry_all = gpd.read_file(Pa_GPkg_1ry)\n",
    "GDF_detail_all = gpd.read_file(Pa_GPkg, layer=detailed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9bffb6",
   "metadata": {},
   "source": [
    "## 2.2. Explore and clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e081a3b",
   "metadata": {},
   "source": [
    "### 2.2.0 Extract coordinates and limit to MdlAa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce16dab",
   "metadata": {},
   "source": [
    "#### Create X & Y columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4159732e",
   "metadata": {},
   "outputs": [],
   "source": [
    "GDF_1ry_all = U.Calc_GDF_XY_start_end_from_Geom(GDF_1ry_all)\n",
    "GDF_detail_all = U.Calc_GDF_XY_start_end_from_Geom(GDF_detail_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dea1628",
   "metadata": {},
   "source": [
    "#### Limit X, Y to model area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af553289",
   "metadata": {},
   "outputs": [],
   "source": [
    "GDF_1ry = GDF_1ry_all[  ( (GDF_1ry_all['Xstart'].between(Xmin, Xmax, inclusive='both') | GDF_1ry_all['Xend'].between(Xmin, Xmax, inclusive='both') ) &\n",
    "              (GDF_1ry_all['Ystart'].between(Ymin, Ymax, inclusive='both') | GDF_1ry_all['Yend'].between(Ymin, Ymax, inclusive='both') ) ) ]\n",
    "GDF_1ry_copy = GDF_1ry.copy()\n",
    "GDF_detail = GDF_detail_all[( (GDF_detail_all['Xstart'].between(Xmin, Xmax, inclusive='both') | GDF_detail_all['Xend'].between(Xmin, Xmax, inclusive='both') ) &\n",
    "              (GDF_detail_all['Ystart'].between(Ymin, Ymax, inclusive='both') | GDF_detail_all['Yend'].between(Ymin, Ymax, inclusive='both') ) ) ]\n",
    "GDF_detail_copy = GDF_detail.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998eddbd",
   "metadata": {},
   "source": [
    "GDF will be our main GDF moving forward. But we'll enrich it with info from the detailed GDF as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d038c6d5",
   "metadata": {},
   "source": [
    "### 2.2.1 Remove columns without much data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fd24dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "GDF_1ry.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d225b428",
   "metadata": {},
   "outputs": [],
   "source": [
    "GDF_detail.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f9cb2b",
   "metadata": {},
   "source": [
    "- A lot of columns in both DFs are either empty, or contain very few values. Those are unecessary, and can be dropped.\n",
    "- GDF contains a few columns that are almost full with values, and then some with only a few values.\n",
    "- Unsurprisingly, GDF_detail has some columns that contain about as many values as the length of the GDF columns that are full. Those are basically information from the main SW network that have been transfered to the detailed lines. We'll check a few to confirm that's true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9e1aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DF_Col_value_counts_grouped(df, percentile_step=10):\n",
    "    \"\"\"Analyze DataFrame columns by non-null value counts, grouped into percentiles.\"\"\"\n",
    "    counts = {col: df[col].count() for col in df.columns}\n",
    "    sorted_counts = sorted(counts.items(), key=lambda x: x[1])\n",
    "    \n",
    "    results = []\n",
    "    n_cols = len(sorted_counts)\n",
    "    \n",
    "    for i in range(0, 100, percentile_step):\n",
    "        start_idx = int(i/100 * n_cols)\n",
    "        end_idx = int((i + percentile_step)/100 * n_cols)\n",
    "        if end_idx > n_cols: end_idx = n_cols\n",
    "        if start_idx == end_idx and end_idx < n_cols: end_idx += 1\n",
    "        \n",
    "        if start_idx < n_cols:\n",
    "            cols_in_range = sorted_counts[start_idx:end_idx]\n",
    "            if cols_in_range:\n",
    "                col_names = [c[0] for c in cols_in_range]\n",
    "                col_counts = [c[1] for c in cols_in_range]\n",
    "                results.append({\n",
    "                    'Percentile_Range': f\"{i}-{i+percentile_step}%\",\n",
    "                    'Min_Values': min(col_counts),\n",
    "                    'Max_Values': max(col_counts),\n",
    "                    'Num_Columns': len(col_names),\n",
    "                    'Columns': col_names\n",
    "                })\n",
    "    \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9d2641",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_Col_value_counts_grouped(GDF_1ry, percentile_step=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65e9d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_Col_value_counts_grouped(GDF_detail, percentile_step=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33094f7c",
   "metadata": {},
   "source": [
    "This confirms that most columns have very few values. We'll drop all columns that have fewer valid values than 10% of the length of the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2a0ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_GDF_Cols_to_keep = [col for col in GDF_1ry.columns if GDF_1ry[col].notnull().sum() >= (0.1 * len(GDF_1ry))]\n",
    "l_GDF_detail_Cols_to_keep = [col for col in GDF_detail.columns if GDF_detail[col].notnull().sum() >= (0.1 * len(GDF_detail))]\n",
    "print(f\"{len(l_GDF_Cols_to_keep)}/{GDF_1ry.shape[1]} columns kept in GDF_1ry.\\n{len(l_GDF_detail_Cols_to_keep)}/{GDF_detail.shape[1]} columns kept in GDF_detail.\")\n",
    "GDF_1ry = GDF_1ry[l_GDF_Cols_to_keep]\n",
    "GDF_detail = GDF_detail[l_GDF_detail_Cols_to_keep]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9059ec2",
   "metadata": {},
   "source": [
    "A lot of columns remain, but their number has reduced a lot. Let's check them, and decide which to keep. We'll proceed with a joint GDF called GDF."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07803d81",
   "metadata": {},
   "source": [
    "### 2.2.2 Manually remove redundant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a5f9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "GDF_1ry.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a1a70b",
   "metadata": {},
   "source": [
    "### 2.2.2 DEBUG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d481e3d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3fc26c02",
   "metadata": {},
   "source": [
    "### 2.2.2 Check if all codes in detail are in 1ry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e336ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "GDF_1ry.loc[~GDF_1ry['CODE'].isin(GDF_detail['CODE'].values)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020533ab",
   "metadata": {},
   "source": [
    "This is strange, I was expecting the detailed GDF to have all the codes from GDF_1ry, but their number is small. I'll check for those items on the map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84eb24f6",
   "metadata": {},
   "source": [
    "- **OVK02041**: is a small ditch, that for whatever reason isn't part of GDF_detail (there is nothing in that exact spot, but it's downstream segment is there). \n",
    "- **OVK00911**: is the end part of OWL37987 in detail.\n",
    "- **OVK02041**: \n",
    "- **OVK02041**: \n",
    "- **OVK02041**: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654daad5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "74ec6f70",
   "metadata": {},
   "source": [
    "### 2.2.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f987e3",
   "metadata": {},
   "source": [
    "## 2.3. Calculate routing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541538f5",
   "metadata": {},
   "source": [
    "#### Identify downstream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875edea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a lookup dictionary from start coordinates to CODE\n",
    "coord_to_id = {(row.Xstart, row.Ystart): (row.CODE, row.OBJECTID) for row in GDF.itertuples()}\n",
    "\n",
    "print(f\"âœ“ Lookup dictionary created with {bold}{len(coord_to_id)}{style_reset} entries.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba42e2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find the downstream ID\n",
    "def get_DStr(row):\n",
    "    end_coords = (row.Xend, row.Yend)\n",
    "    result = coord_to_id.get(end_coords, (0, 0))\n",
    "    return result\n",
    "\n",
    "# Apply the function to create the 'DStr' column\n",
    "GDF[['DStr_code', 'DStr_ID']] = GDF.apply(get_DStr, axis=1, result_type='expand')\n",
    "\n",
    "print(\"âœ“ 'DStr' columns calculated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73dec817",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{round(GDF['DStr_code'].value_counts().max()/GDF.shape[0]*100, 2)} % of DStrs are 0 (i.e. no start coordinates match the end coordinates of the current node).')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30327d3b",
   "metadata": {},
   "source": [
    "The percentage is bigger than expected. Let's investigate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4b50f5",
   "metadata": {},
   "source": [
    "### 2.2.1 Investigate segments that failed to connect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef18cad",
   "metadata": {},
   "source": [
    "#### Check out number of matches/no matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6aa6621",
   "metadata": {},
   "outputs": [],
   "source": [
    "GDF['DStr_match'] = GDF['DStr_code'].isin(GDF['CODE'])\n",
    "GDF['DStr_code'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c8669a",
   "metadata": {},
   "source": [
    "It makes sense that most nodes have just a few upstream nodes. 56 nodes are not connected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9eea89",
   "metadata": {},
   "outputs": [],
   "source": [
    "GDF['DStr_match'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5da81c8",
   "metadata": {},
   "source": [
    "#### Calculate min distance from start to any reach's end and investigate no matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee51767",
   "metadata": {},
   "outputs": [],
   "source": [
    "GDF['min_Dist'] = 0.0\n",
    "GDF.loc[GDF['DStr_code'] == 0, 'min_Dist'] = GDF.loc[GDF['DStr_code'] == 0].apply(lambda row: C.c_Dist(row['Xend'], row['Yend'], GDF['Xstart'], GDF['Ystart']).min(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51039367",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_total_no_match = (GDF['DStr_code'] == 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b04cb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "GDF.loc[GDF['DStr_match']==False,'min_Dist'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6ce2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_Vals = [0.001, 0.1, 1, 10, 100, 1000, 10000]\n",
    "\n",
    "print(f'Out of the {N_total_no_match} segments that do not match:')\n",
    "\n",
    "N_below_Prv, Val_Prv = 0, 0\n",
    "for v in l_Vals:\n",
    "    N_below = (GDF.loc[GDF['DStr_match']==False,'min_Dist'] <= v).sum()\n",
    "    P_below = round(N_below / N_total_no_match * 100, 2)\n",
    "    \n",
    "    sample_gdf = GDF.loc[(GDF['min_Dist'] > Val_Prv) & (GDF['min_Dist'] <= v), ['CODE', 'min_Dist']].sort_values(by='min_Dist')\n",
    "    sample_gdf['Code:min_Dist'] = sample_gdf.apply(lambda row: f\"{row['CODE']}: {row['min_Dist']:8.4f}\", axis=1)\n",
    "    sample_gdf = sample_gdf['Code:min_Dist']\n",
    "    example_nodes = sample_gdf.iloc[:].tolist()\n",
    "\n",
    "    print(f'-{Val_Prv:6} < min_Dist <= {v:5} |N: {N_below:6} (+ {(N_below - N_below_Prv):4}) ({round(P_below,1):5} %) | Codes: {example_nodes}\\n')\n",
    "\n",
    "    N_below_Prv, Val_Prv = N_below, v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3aada2",
   "metadata": {},
   "source": [
    "The total number of segments that do not match is too high to check them all, but we'll check some cases. Check C:\\OD\\WS_Mdl\\Mng\\Mdl_Ipvs.xlsx/WBD_SHP_Eval for more info."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e851d2c9",
   "metadata": {},
   "source": [
    "The ones <1m can be attributed to closing errors, and we can connect them to the closest one via an algorithm.<br>Let's print out the names of the bigger differences, so that we can check them in QGIS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df0d56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "GDF.loc[GDF['min_Dist'].between(10,100, inclusive='right'), ['CODE', 'min_Dist']].sort_values(by='min_Dist').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919d7ccb",
   "metadata": {},
   "source": [
    "- **OVK11551**: -- TO BE CORRECTED -- is an anomaly, as it attaches to a very long feature, far from the final start and end.\n",
    "- **OVK02333**: -- IGNORE -- ends outside the model, so there is no feature to connect to.\n",
    "- **OVK02830** & **OVK02160**: -- IGNORE -- are in the same spot. one of them is straight, the other one circles around, and they both end outside the model.\n",
    "- **OVK01792**: -- IGNORE -- also ends outside the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea85633e",
   "metadata": {},
   "outputs": [],
   "source": [
    "GDF.loc[GDF['min_Dist'].between(100,1000, inclusive='right'), ['CODE', 'min_Dist']].sort_values(by='min_Dist').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952d86a4",
   "metadata": {},
   "source": [
    "- **OVK02242**: --- IGNORE --- ends outside the model and doesn't have any impact on Chaamse Beek \n",
    "- **OVK02238**: --- IGNORE --- ends outside the model and doesn't have any impact on Chaamse Beek \n",
    "- **OVK02121**: --- IGNORE --- ends outside the model and doesn't have any impact on Chaamse Beek \n",
    "- **OVK01807**: --- IGNORE --- ends outside the model and doesn't have any impact on Chaamse Beek \n",
    "- **OVK03078**: --- IGNORE --- ends outside the model and doesn't have any impact on Chaamse Beek \n",
    "- **OVK02887**: --- IGNORE --- ends outside the model and doesn't have any impact on Chaamse Beek (final Mark outled outside MdlAa) \n",
    "- **OVK02933**: --- IGNORE --- ends outside the model and doesn't have any impact on Chaamse Beek \n",
    "- **OVK01682**: --- IGNORE --- ends outside the model and doesn't have any impact on Chaamse Beek \n",
    "- **OVK00416**: --- IGNORE --- ends outside the model and doesn't have any impact on Chaamse Beek \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a67c03c",
   "metadata": {},
   "source": [
    "### 2.2.2 Edit connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc84c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the 'multiple_close' column with empty strings\n",
    "GDF['multiple_close'] = \"\"\n",
    "\n",
    "# Select rows to correct\n",
    "rows_to_correct = GDF[(GDF['DStr_code'] == 0) & (GDF['min_Dist'] < 1)].copy()\n",
    "\n",
    "print(f\"Found {len(rows_to_correct)} segments with no downstream connection and a potential connection within 1m.\")\n",
    "\n",
    "# Create a Series of all start coordinates for faster access\n",
    "all_starts_x = GDF['Xstart']\n",
    "all_starts_y = GDF['Ystart']\n",
    "all_codes = GDF['CODE']\n",
    "all_object_ids = GDF['OBJECTID']\n",
    "\n",
    "corrected_count = 0\n",
    "multiple_count = 0\n",
    "\n",
    "# Loop through the rows that need correction\n",
    "for index, row in rows_to_correct.iterrows():\n",
    "    # Calculate distances from the current row's end point to all other rows' start points\n",
    "    distances = C.c_Dist(row['Xend'], row['Yend'], all_starts_x, all_starts_y)\n",
    "    \n",
    "    # Find segments where the distance is less than 1m\n",
    "    close_mask = (distances < 1) & (GDF.index != index) # Exclude self\n",
    "    close_segments_codes = all_codes[close_mask].tolist()\n",
    "    close_segments_object_ids = all_object_ids[close_mask].tolist()\n",
    "    \n",
    "    if len(close_segments_codes) == 1:\n",
    "        # If there is exactly one close segment, update both 'DStr_code' and 'DStr_ID'\n",
    "        GDF.loc[index, 'DStr_code'] = close_segments_codes[0]\n",
    "        GDF.loc[index, 'DStr_ID'] = close_segments_object_ids[0]\n",
    "        corrected_count += 1\n",
    "    elif len(close_segments_codes) > 1:\n",
    "        # If there are multiple close segments, store as comma-separated string\n",
    "        GDF.loc[index, 'multiple_close'] = \", \".join(close_segments_codes)\n",
    "        multiple_count += 1\n",
    "\n",
    "print(f\"âœ“ Corrected {corrected_count} segments by updating 'DStr_code' and 'DStr_ID'.\")\n",
    "print(f\"âœ“ Found {multiple_count} segments with multiple potential connections (stored in 'multiple_close').\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a824ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "GDF.loc[GDF['multiple_close']!='', ['CODE', 'multiple_close']].sort_values(by='multiple_close', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3692cca0",
   "metadata": {},
   "source": [
    "This is an upstremost segment that's really small, so it doesn't matter much where it connects to. We'll connect it to one of the two downstream segments at random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad4c4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "GDF.loc[GDF['CODE']=='OVK00976', ['DStr_code', 'DStr_ID']] = ['OVK00977', GDF.loc[GDF['CODE']=='OVK00977', 'OBJECTID'].values[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68b631e",
   "metadata": {},
   "source": [
    "Let's also fix OVK11551, which connects to that very long segment (OVK03013), far from either end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bf67ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "GDF.loc[GDF['CODE']=='OVK11551', ['DStr_code', 'DStr_ID']] = ['OVK03013', GDF.loc[GDF['CODE']=='OVK03013', 'OBJECTID'].values[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a4aee5",
   "metadata": {},
   "source": [
    "## 2.3. Explore GDF to fill SFRmaker input requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493f938c",
   "metadata": {},
   "source": [
    "### 2.3.0 Custom Hydrography Required Fields"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2794d0",
   "metadata": {},
   "source": [
    "Below, we'll match the shapefile attributes to the SFRmaker requirements as per SFRmaker manual ( https://doi-usgs.github.io/sfrmaker/latest/inputs.html):\n",
    "\n",
    "**Custom hydrography** <br>\n",
    "Any Polyline shapefile can be supplied in lieu of NHDPlus, but it must have the following columns, as shown in the second example:\n",
    "1. **flowlines_file**: path to shapefile\n",
    "2. **id_column**: unique identifier for each polyline\n",
    "3. **routing_column**: downstream connection (ID), 0 if none\n",
    "4. **width1_column**: channel width at start of line, in attr\\_length\\_units (optional)\n",
    "5. **width2_column**: channel width at end of line, in attr_length_units (optional)\n",
    "6. **up_elevation_column**: streambed elevation at start of line, in attr_height_units\n",
    "7. **dn_elevation_column**: streambed elevation at end of line, in attr_height_units\n",
    "8. **name_column**: stream name (optional)\n",
    "9. **attr_length_units**: channel width units\n",
    "10. **attr_height_units**: streambed elevation units"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eca8d4f",
   "metadata": {},
   "source": [
    "Here are the columns of the GDF, so we can match them to the SFRmaker requirements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd29ad75",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, col in enumerate(GDF.columns):\n",
    "    print(f\"{i}: {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921bff08",
   "metadata": {},
   "source": [
    "### 2.3.2 Explore GDF fields"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7fc5b7",
   "metadata": {},
   "source": [
    "#### 2. & 3. will most likely be 'OBJECTID' & 'DStr_ID'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231e7e60",
   "metadata": {},
   "source": [
    "But let's ensure there are no nulls and use the .describe() method to check more details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9a763c",
   "metadata": {},
   "outputs": [],
   "source": [
    "GDF[['OBJECTID', 'DStr_ID']].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf2cb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "GDF[['CODE', 'OBJECTID', 'DStr_code', 'DStr_ID']].describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cb0a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "GDF.loc[GDF['DStr_ID']==0, 'DStr_ID'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9153e150",
   "metadata": {},
   "source": [
    "Great. No nuls and 13 DStr_codes with no downstream connection ('DStr'==0) (9 with min_Dist between 100 and 1000, + 5 with min_Dist betwen 10 and 100, -1, that we connected to the meandering long segment)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87cb3a9e",
   "metadata": {},
   "source": [
    "#### 4. & 5. Widths (at start and end)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb12d224",
   "metadata": {},
   "source": [
    "There are 4 candidates for those fields:\n",
    "- 'WS_BODEMBREEDTE_L' (channel bed width)\n",
    "- 'WS_BODBREE_ACCPROF_LI_L' (Accepted profile bottom width (left) (m))\n",
    "- 'WS_BODBREE_ACCPROF_RE_L' (Accepted profile bottom width (right) (m))\n",
    "- 'WS_VW_BODEMBREEDTE_L' (Prescribed bottom width (voorwaarde) (m))\n",
    "\n",
    "Let's compare the values. We can probably just use WS_BODEMBREEDTE. I think it has no/fewer nuls and the values are similar to the other 3 fields. Then we can upgrade. But let's have a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa23b772",
   "metadata": {},
   "outputs": [],
   "source": [
    "GDF[['CODE', 'OBJECTID', 'WS_BODEMBREEDTE_L', 'WS_BODBREE_ACCPROF_LI_L', 'WS_BODBREE_ACCPROF_RE_L', 'WS_VW_BODEMBREEDTE_L']].describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567fc106",
   "metadata": {},
   "outputs": [],
   "source": [
    "GDF.loc[ GDF['WS_BODBREE_ACCPROF_LI_L'].notna() | GDF['WS_BODBREE_ACCPROF_RE_L'].notna() | GDF['WS_VW_BODEMBREEDTE_L'].notna(),\n",
    "         ['CODE', 'WS_BODEMBREEDTE_L', 'WS_BODBREE_ACCPROF_LI_L', 'WS_BODBREE_ACCPROF_RE_L', 'WS_VW_BODEMBREEDTE_L']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97f392f",
   "metadata": {},
   "source": [
    "'WS_BODBREE_ACCPROF_LI_L' & 'WS_BODBREE_ACCPROF_RE_L' are not the bottom of the channel. This is clear if we masure the channel width in QGIS. So we won't use those.<br>\n",
    "'WS_VW_BODEMBREEDTE_L' only has nulls, so we won't use that either.<br>\n",
    "**So we'll just use 'WS_BODEMBREEDTE_L'.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5be5b8",
   "metadata": {},
   "source": [
    "#### 6. & 7. Upstream and downstream elevations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd44a165",
   "metadata": {},
   "outputs": [],
   "source": [
    "GDF[[ 'CODE', 'WS_BH_BOVENSTROOMS_L', 'WS_BH_BENEDENSTROOMS_L']].describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ad2d63",
   "metadata": {},
   "source": [
    "No nulls + the percentiles make sense.ðŸŸ¢<br>\n",
    "Let's make sure the UStr is always higher than the DnStr.<br>\n",
    "Then let's print out some values to check in QGIS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e66c159",
   "metadata": {},
   "outputs": [],
   "source": [
    "(GDF['WS_BH_BOVENSTROOMS_L'] <= GDF['WS_BH_BENEDENSTROOMS_L']).sum(), (GDF['WS_BH_BOVENSTROOMS_L'] < GDF['WS_BH_BENEDENSTROOMS_L']).sum(), (GDF['WS_BH_BOVENSTROOMS_L'] > GDF['WS_BH_BENEDENSTROOMS_L']).sum(), GDF.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2cffa3",
   "metadata": {},
   "source": [
    "For 98 segments (out of 591), the UStr Elv is <= the DStr Elv. This is not good. We'll have to fix this. ðŸ”´<br>\n",
    "Only 24/99 have DStr < UStr , the rest are equal. = will be corrected by SFR itself (as far as I know), so no action is required for those."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd1d804",
   "metadata": {},
   "source": [
    "##### Let's print out some CODEs where =, to check in QGIS. *(We don't really need to, I'm just curious)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418d90d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "GDF_Elv = GDF[['CODE', 'WS_BH_BOVENSTROOMS_L', 'WS_BH_BENEDENSTROOMS_L', 'DStr_code', 'DStr_ID']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3d1f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "GDF_Elv['Diff'] = GDF_Elv['WS_BH_BOVENSTROOMS_L'] - GDF_Elv['WS_BH_BENEDENSTROOMS_L']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53c1541",
   "metadata": {},
   "outputs": [],
   "source": [
    "GDF_Elv.loc[GDF_Elv['Diff'] == 0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d929faa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "GDF_Elv.loc[ GDF_Elv['Diff'] < 0 ].sort_values(by='Diff', ascending=True).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136a5b19",
   "metadata": {},
   "source": [
    "##### Let's see if any of the problematic segments have multiple UStr segments. That would make a solution harder to implement.<br>\n",
    "*(if there is only 1 UStr segment, the DStr Elv of the UStr segment can be modified to allow the UStr Elv of the current segmet to be increased as well, but if there are multiple, this becomes more complicated)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4508d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_problematic = GDF_Elv.loc[ GDF_Elv['Diff'] < 0, 'CODE'].tolist()\n",
    "for S in l_problematic:\n",
    "    sum = (GDF['DStr_code']==S).sum()\n",
    "    if sum > 1:\n",
    "        print(S, sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce539682",
   "metadata": {},
   "source": [
    "There are multiple segments with more than 1 UStr segment. We'll have to consider this when designing the elevation correction logic. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3845820b",
   "metadata": {},
   "source": [
    "##### Elv correction algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f4d1ac",
   "metadata": {},
   "source": [
    "We'll design an algorithm to fix those with <. Those with = will be fixed by SFR itself (hopefully). The following abbreviations are useful for explaining the concept:\n",
    "- A: DStr Elv of DStr segment\n",
    "- B: UStr Elv of DStr segment\n",
    "- C: DStr Elv of current segment\n",
    "- D: UStr Elv of current segment\n",
    "- F: DStr Elv of UStr segment(s)\n",
    "\n",
    "Here is the idea behind the algorithm:\n",
    "1. If **C > D & B <= D** :<br>\n",
    "-> Set **C = D**\n",
    "2. If **C > D & B > D** :<br>\n",
    "-> Set **C = D**. Set **B = D**\n",
    "3. If **C <= D** :<br>\n",
    "-> **No action**.\n",
    "\n",
    "Repeat till there are no segments with C < D.\n",
    "\n",
    "When there is no downstream segment, we apply the logic used in case 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe8a361",
   "metadata": {},
   "outputs": [],
   "source": [
    "GDF_Elv = GDF_Elv.merge(GDF[['CODE', 'WS_BH_BOVENSTROOMS_L', 'WS_BH_BENEDENSTROOMS_L']], left_on='DStr_code', right_on='CODE', suffixes=('', '_DStr'), how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c724682",
   "metadata": {},
   "outputs": [],
   "source": [
    "GDF_Elv[['A', 'B']] = GDF_Elv[['WS_BH_BENEDENSTROOMS_L_DStr', 'WS_BH_BOVENSTROOMS_L_DStr']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8939f588",
   "metadata": {},
   "outputs": [],
   "source": [
    "GDF_Elv[['C', 'D']] = GDF_Elv[['WS_BH_BENEDENSTROOMS_L', 'WS_BH_BOVENSTROOMS_L']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffa1e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "GDF_Elv[GDF_Elv['B'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6938d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "init = GDF_Elv.loc[ GDF_Elv['CODE']=='OVK02121', 'B' ].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be58f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_elevations(row):\n",
    "    if row['C'] <= row['D']: # If UStr Elv <= DStr Elv, no adjustment needed\n",
    "        return row['B'], row['C']\n",
    "    elif (row['C'] > row['D']) and (pd.isna(row['B'])): # If UStr Elv <= DStr Elv but DStr Elv is missing (OuFl segment), set both to DStr Elv\n",
    "        return pd.NA, row['D']\n",
    "    elif (row['C'] > row['D']) and (row['B'] <= row['D']):\n",
    "        return row['B'], row['D']\n",
    "    elif (row['C'] > row['D']) and (row['B'] > row['D']):\n",
    "        return row['D'], row['D']\n",
    "    else:\n",
    "        # Default case - should not happen, but ensures function always returns a tuple\n",
    "        return row['B'], row['C']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091c8562",
   "metadata": {},
   "outputs": [],
   "source": [
    "GDF_Elv[['B_', 'C_']] = GDF_Elv.apply(adjust_elevations, axis=1, result_type='expand')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324f1d62",
   "metadata": {},
   "source": [
    "I'm worried consequtive segments might be problematic. Let's check if there are any."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899beccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "GDF_Elv_unfixed = GDF_Elv[ (GDF_Elv['Diff']<0)]\n",
    "consequtive = GDF_Elv_unfixed.loc[GDF_Elv_unfixed['DStr_code'].isin(GDF_Elv_unfixed['CODE']), 'DStr_code']\n",
    "GDF_Elv_unfixed.loc[ (GDF_Elv_unfixed['CODE'].isin(consequtive)) | (GDF_Elv_unfixed['DStr_code'].isin(consequtive)), ['CODE', 'DStr_code', 'A', 'B', 'B_', 'C', 'C_', 'D']].sort_values(by='D').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3283da8e",
   "metadata": {},
   "source": [
    "Consequtive ok. Let's check if there is a drop in Elv (positive slope) for each segment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18120c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "GDF_Elv.loc[ GDF_Elv['D'] - GDF_Elv['C_'] < 0 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2378578",
   "metadata": {},
   "source": [
    "Cool, no segments without any drop in Elv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9534d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "GDF_Elv['segment_drop'] = GDF_Elv['D'] - GDF_Elv['C_']\n",
    "GDF_Elv['DStr_drop'] = GDF_Elv['C_'] - GDF_Elv['B']\n",
    "GDF_Elv.loc[ GDF_Elv['C_'] - GDF_Elv['B_'] < 0 , ['CODE', 'DStr_code', 'A', 'B', 'B_', 'C', 'C_', 'D', 'segment_drop', 'DStr_drop'] ].sort_values(by='DStr_drop').reset_index(drop=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4632de",
   "metadata": {},
   "source": [
    "There are **quite a few** segments where C_ > B!!! SFRmaker might fix this. If not, I'll come back and fix it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7075e9b",
   "metadata": {},
   "source": [
    "#### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e335bf92",
   "metadata": {},
   "source": [
    "**Custom hydrography** <br>\n",
    "Any Polyline shapefile can be supplied in lieu of NHDPlus, but it must have the following columns, as shown in the second example:\n",
    "1. flowlines_file: path to shapefile <br>\n",
    "    -> **Pa_GPkg_1ry** <br>\n",
    "2. id_column: unique identifier for each polyline <br>\n",
    "    -> **OBJECTID** <br>\n",
    "3. routing_column: downstream connection (ID), 0 if none <br>\n",
    "    -> **DStr_ID** <br>\n",
    "4. width1_column: channel width at start of line, in attr\\_length\\_units (optional) <br>\n",
    "    -> **WS_BODEMBREEDTE_L** <br>\n",
    "5. width2_column: channel width at end of line, in attr_length_units (optional) <br>\n",
    "    -> **WS_BODEMBREEDTE_L** <br>\n",
    "6. up_elevation_column: streambed elevation at start of line, in attr_height_units <br>\n",
    "    -> **WS_BH_BOVENSTROOMS_L** <br>\n",
    "7. dn_elevation_column: streambed elevation at end of line, in attr_height_units <br>\n",
    "    -> **WS_BH_BENEDENSTROOMS_L** <br>\n",
    "8. name_column: stream name (optional) <br>\n",
    "    -> **CODE** <br>\n",
    "9. attr_length_units: channel width units <br>\n",
    "    -> **'m'** <br>\n",
    "10. attr_height_units: streambed elevation units <br>\n",
    "    -> **'m'** <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4796367",
   "metadata": {},
   "outputs": [],
   "source": [
    "GDF_Elv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8559b003",
   "metadata": {},
   "outputs": [],
   "source": [
    "GDF_copy = GDF.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14a4180",
   "metadata": {},
   "outputs": [],
   "source": [
    "GDF = GDF_copy.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3697e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "GDF = GDF.merge( GDF_Elv[['CODE', 'C_', 'D']], on='CODE', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53020b74",
   "metadata": {},
   "source": [
    "## 2.4 Generate SFRmaker lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b41c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "GDF['width2'] = GDF['WS_BODEMBREEDTE_L'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ea3ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = sfr.Lines.from_dataframe(df=GDF.copy(), # .copy() to avoid GDF columns being renamed by function (this feels like a bug to me)\n",
    "    id_column='OBJECTID',\n",
    "    routing_column='DStr_ID',\n",
    "    width1_column='WS_BODEMBREEDTE_L', width2_column='width2',\n",
    "    dn_elevation_column='C_',\n",
    "    up_elevation_column='D',\n",
    "    name_column='CODE',\n",
    "    width_units='m',\n",
    "    height_units='m',\n",
    "    crs=GDF.crs\n",
    "    #    shapefile=Pa_GPkg_1ry_SHP_SFR,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093af5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_lines = lines.df\n",
    "U.DF_info(lines.df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2bf09c2",
   "metadata": {},
   "source": [
    "# 3. Connect SFR to MF6 model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1ea38c",
   "metadata": {},
   "source": [
    "## 3.0. Create SFR_grid item"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae1c448",
   "metadata": {},
   "source": [
    "### 3.0.0 Initiate parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56cfab97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sfr.StructuredGrid directly from MF6_DIS (DataFrame approach) #666 This cell and the cells below it can be combined into a function to read in a MF6_DIS (imod) object, and return a DF (GDF_grid) with the grid and geometry. \n",
    "DS = MF6_DIS.dataset\n",
    "N_L, N_R, N_C = DS.dims['layer'], DS.dims['y'], DS.dims['x']\n",
    "dx, dy = abs(float(DS.coords['dx'].values)), abs(float(DS.coords['dy'].values))\n",
    "Ls, Xs, Ys = DS.coords['layer'].values, DS.coords['x'].values, DS.coords['y'].values\n",
    "X_Ogn, Y_Ogn = Xs[0] - dx/2, Ys[0] + dy/2  # Upper-left corner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fb910e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct TOP, BOT. TOP array: 1st layer from DS['top'], rest from DS['bottom'][::-1] with layer+1\n",
    "TOPs = np.zeros((N_L, N_R, N_C))\n",
    "TOPs[0] = DS['top'].values\n",
    "TOPs[1:] = DS['bottom'].sel(layer=range(1, N_L))\n",
    "BOTs = DS['bottom'].values  # Shape: (N_L, N_R, N_C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a81f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create full 3D grid indices\n",
    "k, i, j = np.meshgrid(range(N_L), range(N_R), range(N_C), indexing='ij')\n",
    "k, i, j = k.ravel(), i.ravel(), j.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30f30c0",
   "metadata": {},
   "source": [
    "### 3.0.1 Prepare GDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5534d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "GDF_grid = gpd.GeoDataFrame({\n",
    "    'k': k,\n",
    "    'i': i,\n",
    "    'j': j, \n",
    "    'node': range(N_L * N_R * N_C),\n",
    "    'isfr': 1,  # All cells can potentially have SFR # if function is made out of this, this needs to be removed and added to the DF after the function has run.\n",
    "    'top': TOPs.ravel(),\n",
    "    'bottom': BOTs.ravel(),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7273f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = GDF_grid['k'].eq(0)\n",
    "i_L0 = GDF_grid.loc[mask, 'i'].to_numpy()\n",
    "j_L0 = GDF_grid.loc[mask, 'j'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc02f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "xmin = X_Ogn + j_L0*dx\n",
    "xmax = X_Ogn + (j_L0+1)*dx\n",
    "ymin = Y_Ogn - (i_L0+1)*dy\n",
    "ymax = Y_Ogn - i_L0*dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c04b0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "L0_geom = [box(x0, y0, x1, y1) for x0, y0, x1, y1 in zip(xmin, ymin, xmax, ymax)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f707b763",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in GDF_grid['k'].unique():\n",
    "    GDF_grid.loc[GDF_grid['k'] == k, 'geometry'] = L0_geom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92691bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "GDF_grid = GDF_grid.set_geometry('geometry', crs=DS.rio.crs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43d84b1",
   "metadata": {},
   "source": [
    "### 3.0.2 Identify deepest SFR layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75ef3d1",
   "metadata": {},
   "source": [
    "The reason we're doing this is that the model has too many Ls and it takes a very long time to run the SFR functions with all of them. So we'll find the deepest L that has any part of the stream network in it, and **we'll only use up to that layer for the SFR grid.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ed7853",
   "metadata": {},
   "outputs": [],
   "source": [
    "for L in range(BOTs.shape[0]):\n",
    "    L_BOT_min = BOTs[L].min()\n",
    "    L_BOT_max = BOTs[L].max()\n",
    "    print(L+1, f\"|{L_BOT_min:8.2f} |\", f\"{L_BOT_max:8.2f} |\")\n",
    "    if L_BOT_min > DF_lines['elevdn'].min():\n",
    "        SFR_deepest_L = L+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aee6560",
   "metadata": {},
   "outputs": [],
   "source": [
    "SFR_deepest_L"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438b8fa1",
   "metadata": {},
   "source": [
    "### 3.0.3 Create SFR grid(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbdf43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "SFR_grid = sfr.StructuredGrid(GDF_grid.loc[GDF_grid['k'] <= SFR_deepest_L-1], crs=G.crs) # -1 cause grid k starts at 0, L at 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f544790a",
   "metadata": {},
   "outputs": [],
   "source": [
    "SFR_grid_L1 = sfr.StructuredGrid(GDF_grid.loc[GDF_grid['k'] == 0], crs=G.crs) # Extract layer 1 (k=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874b2665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check what type of object and its basic info without triggering full repr\n",
    "print(f\"Type: {type(SFR_grid)}\")\n",
    "print(f\"SFR_grid object created: {SFR_grid is not None}\")\n",
    "\n",
    "# Check if it has expensive methods for representation\n",
    "print(f\"Available methods: {[method for method in dir(SFR_grid) if not method.startswith('_')][:10]}\")\n",
    "\n",
    "# Try to get basic info without full representation\n",
    "try:\n",
    "    print(f\"Grid shape info: {hasattr(SFR_grid, 'shape')}\")\n",
    "    if hasattr(SFR_grid, 'nlay'):\n",
    "        print(f\"Number of layers: {SFR_grid.nlay}\")\n",
    "    if hasattr(SFR_grid, 'nrow'):\n",
    "        print(f\"Number of rows: {SFR_grid.nrow}\")\n",
    "    if hasattr(SFR_grid, 'ncol'):\n",
    "        print(f\"Number of cols: {SFR_grid.ncol}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error getting basic info: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490b38bc",
   "metadata": {},
   "source": [
    "## 3.2. SFRdata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a515c48e",
   "metadata": {},
   "source": [
    "### 3.2.0 Create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bda520",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = sfr.Lines.from_dataframe(df=GDF.copy(), # .copy() to avoid GDF columns being renamed by function (this feels like a bug to me)\n",
    "    id_column='OBJECTID',\n",
    "    routing_column='DStr_ID',\n",
    "    width1_column='WS_BODEMBREEDTE_L', width2_column='width2',\n",
    "    dn_elevation_column='C_',\n",
    "    up_elevation_column='D',\n",
    "    name_column='CODE',\n",
    "    width_units='m',\n",
    "    height_units='m',\n",
    "    crs=GDF.crs\n",
    "    #    shapefile=Pa_GPkg_1ry_SHP_SFR,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e4d066",
   "metadata": {},
   "outputs": [],
   "source": [
    "SFR_data = lines.to_sfr(grid=SFR_grid_L1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55633e76",
   "metadata": {},
   "source": [
    "### 3.2.1 Explore DF_reach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9f57e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_reach = SFR_data.reach_data.copy()\n",
    "DF_reach[['k', 'i', 'j']] = DF_reach[['k', 'i', 'j']] + 1 # convert to 1-based indexing for reviewing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2a50a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_reach.describe() #include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59200f38",
   "metadata": {},
   "source": [
    "Some comments regarding DF_reaches:\n",
    "- We have a large **number of reaches** (rno.max()=7819), and all columns have the same number of valid values, which is good.\n",
    "- **k** wasn't filled properly. We need to use the assign_layer function to fix this. **Surprise...<br>There are 2...<br>\n",
    "<t> sfrmaker.sfrdata.assign_layers <br>\n",
    "<t> sfrmaker.utils.assign_layers <br>\n",
    "We'll use the latter, where we can use BOTs. The other one requires a full loaded flopy model. <t>**\n",
    "- **j** is within range, so it was probably calculated correctly.\n",
    "- **iseg** makes sense. **ireach** is the reach number within the segment (according to copilot), seems feasible.\n",
    "- **width** has a few values that are too big. Let's print them out to check in QGIS.\n",
    "- **rchlen, slope, strtop** all make sense.\n",
    "- **strthick** is 1 everywhere. We need to edit this, based on some sort of assumption and the conductance value of the equivalent RIV item. Let's start with strthick=0.1 (cause 1m is too much).\n",
    "- **strhc1**, **thts**, **thti**, **eps** & **uhc** are not used as far as I know.\n",
    "- **outreach** seems iffy, as it's float, while I was expecting an int.\n",
    "- how can **asum** be negative?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34535a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_reach['strthick'] = 0.1  # Set a default streambed thickness of 0.1 m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65941b7",
   "metadata": {},
   "source": [
    "#### Explore width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0f702f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_reach.loc[:, ['rno', 'outreach', 'iseg', 'outseg', 'node', 'k', 'i', 'j', 'name', 'rchlen', 'width', 'strtop', 'strthick', 'asum']].sort_values(by=['width', 'i', 'j'], ascending=[False, True, True])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909d0649",
   "metadata": {},
   "source": [
    "Fortunately, it's just 1 feature, which I'll correct manually RN. The rest seem fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479a9ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_reach.loc[DF_reach['name'] == 'OVK20449', 'width'] = 0.9 # It's upstream reach is 0.8, it's downstream is 1m. Let's set is as the average for now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c64e80",
   "metadata": {},
   "source": [
    "### 3.2.2 Assign the correct layers - k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56bc87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_reach[['k', 'i', 'j']] = DF_reach[['k', 'i', 'j']] - 1 # convert to 0-based indexing for utils_assign_layers function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c41ea0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "reach_Ls, strtps = sfr.utils.assign_layers(reach_data=DF_reach, botm_array=BOTs, pad=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2274bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_reach['k'] = reach_Ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5192d91f",
   "metadata": {},
   "source": [
    "### 3.2.3 Check \n",
    "Examples to check if segments were connected to the right cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2321af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, seg in enumerate(DF_reach['name'].unique()[:10]):\n",
    "    print(i+1, seg, DF_reach.loc[DF_reach['name']==seg, 'name'].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4906e616",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_reach[['k', 'i', 'j']] = DF_reach[['k', 'i', 'j']] + 1 # convert to 1-based indexing for reviewing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef23b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_reach.loc[DF_reach['name'] == 'OVK01451', ['rno', 'outreach', 'iseg', 'outseg', 'node', 'k', 'i', 'j', 'name', 'rchlen', 'width', 'strtop', \n",
    "                                              'strthick', 'asum']].sort_values(by=['i', 'j'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc9aa4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_reach.loc[DF_reach['name'] == 'OVK02048', ['rno', 'outreach', 'iseg', 'outseg', 'node', 'k', 'i', 'j', 'name', 'rchlen', 'width', 'strtop', \n",
    "                                              'strthick', 'asum']].sort_values(by=['name', 'j', 'i'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc36d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_reach.loc[DF_reach['name'] == 'OVK20466', ['rno', 'outreach', 'iseg', 'outseg', 'node', 'k', 'i', 'j', 'name', 'rchlen', 'width', 'strtop', \n",
    "                                              'strthick', 'asum']].sort_values(by=['name', 'j', 'i'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e754ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_reach[['k', 'i', 'j']] = DF_reach[['k', 'i', 'j']] - 1 # convert to 0-based indexing for SFRmaker operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99082b4",
   "metadata": {},
   "source": [
    "### 3.2.4 Apply RIV conductance to DF_reach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9449c15b",
   "metadata": {},
   "source": [
    "##### Calculate Default Conductance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fddbeef",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_RC = DF_reach.copy()[['rno', 'name', 'k', 'i', 'j', 'iseg', 'outseg', 'rchlen', 'width', 'strtop', 'strthick', 'strhc1', 'asum']]\n",
    "DF_RC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25b0253",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_RC['Cond'] = DF_RC['width'] * DF_RC['rchlen'] * DF_RC['strhc1'] / DF_RC['strthick']\n",
    "DF_RC.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f05cc1",
   "metadata": {},
   "source": [
    "##### Import RIV Cond shapefiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d0bbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Pa_Cond = r\"C:\\OD\\WS_Mdl\\models\\NBr\\In\\RIV\"\n",
    "l_Pa_Cond = [i for i in U.LD(Pa_Cond) if ('Cond' in i) and i.lower().endswith('.idf')]\n",
    "l_Pa_Cond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939e690e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import imod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28756438",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_Pa_Cond = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d8169d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for Pa in l_Pa_Cond:\n",
    "    d_Pa_Cond[Pa] = imod.idf.open(PJ(Pa_Cond, Pa))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d98c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "A_key = list(d_Pa_Cond.keys())[1]\n",
    "A_whole = d_Pa_Cond[A_key]\n",
    "A = A_whole.sel(x=slice(Xmin, Xmax), y=slice(Ymax, Ymin))\n",
    "print(A_key)\n",
    "A.plot.imshow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a3f712",
   "metadata": {},
   "outputs": [],
   "source": [
    "B_key = list(d_Pa_Cond.keys())[0]\n",
    "B_whole = d_Pa_Cond[B_key]\n",
    "B = B_whole.sel(x=slice(Xmin, Xmax), y=slice(Ymax, Ymin))\n",
    "print(B_key)\n",
    "B.plot.imshow()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9283fa98",
   "metadata": {},
   "source": [
    "We've loaded the main RIV cond as A, and the detailed as B. We'll use the average for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f98a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create union array C: average where both valid, single value where only one valid\n",
    "C = xr.where(~np.isnan(A) & ~np.isnan(B), (A + B) / 2,  # Both valid: average\n",
    "             xr.where(~np.isnan(A), A, B))                # Only one valid: use that one\n",
    "\n",
    "print(f\"Union array C: {(~np.isnan(C)).sum().values} valid values, sum = {C.sum(skipna=True).values}\")\n",
    "C.plot.imshow()\n",
    "plt.title('Array C: Union of A and B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1cc778",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_RC['RIV_Cond'] = DF_RC['Cond'].copy() # Apply conductance matching to DF_RC using array C. Start with copy of existing Cond values as fallback\n",
    "\n",
    "C_DF_RC = C.values[DF_RC['i'].values, DF_RC['j'].values] # Get array values for all i,j coordinates at once (vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feebe6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace only where array has valid (non-NaN) values\n",
    "valid_mask_RC = ~np.isnan(C_DF_RC)\n",
    "DF_RC.loc[valid_mask_RC, 'RIV_Cond'] = C_DF_RC[valid_mask_RC]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9fdd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"DF_RC conductance matching results:\")\n",
    "print(f\"Replaced {valid_mask_RC.sum()} values out of {len(DF_RC)} total rows ({valid_mask_RC.sum()/len(DF_RC)*100:.1f}%)\")\n",
    "print(f\"Original Cond: min={DF_RC['Cond'].min():.3f}, max={DF_RC['Cond'].max():.3f}\")\n",
    "print(f\"New RIV_Cond: min={DF_RC['RIV_Cond'].min():.3f}, max={DF_RC['RIV_Cond'].max():.3f}\")\n",
    "\n",
    "# Check how many values actually changed\n",
    "changed_values_RC = (DF_RC['Cond'] != DF_RC['RIV_Cond'])\n",
    "print(f\"Values that changed: {changed_values_RC.sum()} out of {len(DF_RC)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f265b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_RC['K_RIV'] = DF_RC['RIV_Cond'] * DF_RC['strthick'] / (DF_RC['width'] * DF_RC['rchlen'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafecc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_RC['Cond_Diff'] = DF_RC['RIV_Cond'] - DF_RC['Cond']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ab456f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_RC.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c618f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_reach['strhc1'] = DF_RC['K_RIV'] # Set it back to DF_reach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9faa5f4b",
   "metadata": {},
   "source": [
    "### 3.2.5 Explore segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e903c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_Sgm = SFR_data.segment_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcef1abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_Sgm.iloc[:].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d83bb2",
   "metadata": {},
   "source": [
    "Most columns aren't interesting. Let's plot the interesting ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d603ed7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_Sgm[[\"nseg\", \"outseg\", \"roughch\", \"elevup\", \"elevdn\", \"width1\", \"width2\", ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adebbc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "(DF_Sgm['width1'] == DF_Sgm['width1']).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e6c30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "(DF_Sgm['elevup'] >= DF_Sgm['elevdn']).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af18ca9a",
   "metadata": {},
   "source": [
    "We can see:\n",
    "- the roughness values are all the same (default) - **OK**\n",
    "- downstream elevation is always lower than (or equal to) upstream - **OK**\n",
    "- the widths seem to be the ones read from the shapefile - **OK**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fdd3d0c",
   "metadata": {},
   "source": [
    "### 3.2.6 Add OBS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a482922",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(SFR_data.add_observations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea06e904",
   "metadata": {},
   "outputs": [],
   "source": [
    "Pa_SFR_OBS_In = PJ(d_Pa['In'], f'OBS/SFR/NBr35/NBr35_SFR_OBS_Pnt.csv') #666 Should be PJ(d_Pa['In'], f'OBS/SFR/{MdlN}/{MdlN}_SFR_OBS_Pnt.csv')\n",
    "DF_SFR_OBS = pd.read_csv(Pa_SFR_OBS_In)\n",
    "DF_SFR_OBS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394c3936",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in DF_SFR_OBS.iterrows(): # Have to add them one by one, otherwise it groups them by reach and only keeps the 1st one. This is an SFRmaker bug, I can fix that later and make a pull request.\n",
    "    SFR_data.add_observations(\n",
    "        pd.DataFrame(row).T,\n",
    "        x_location_column='x',  \n",
    "        y_location_column='y',\n",
    "        obstype_column='obstype',\n",
    "        obsname_column='site_no'\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8ffb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "SFR_data.observations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d47b08",
   "metadata": {},
   "source": [
    "### 3.2.7 Run diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe18b763",
   "metadata": {},
   "outputs": [],
   "source": [
    "SFR_data.run_diagnostics(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43b71e4",
   "metadata": {},
   "source": [
    "Most checks passed, except for:\n",
    "1. Checking reach_data for downstream rises in streambed elevation...<br>68 reaches encountered with strtop < strtop of downstream reach. Let's see if this causes a problem.\n",
    "2. Checking for model cells with multiple non-zero SFR conductances...\n",
    "565 model cells with multiple non-zero SFR conductances found.\n",
    "This can be fixed easily with one of the SFRdata options. We'll come here if it causes an error in the Sim.\n",
    "3. floppy Mdl not connected to SFRdata means:<br>\n",
    "    3.1 Cannot check reach proximities\n",
    "    3.2 Cannot check streambed elevations against cell bottom elevations. This shouldn't be a problem as the assign_layers function uses strbedthck (to assign k).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f187064f",
   "metadata": {},
   "outputs": [],
   "source": [
    "GDF_Elv.loc[ GDF_Elv['D'] - GDF_Elv['B_'] < 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec1e762",
   "metadata": {},
   "source": [
    "There are fewer entries in the GDF_Elv where the DStr Elv > UStr Elv, but this DF contains segments, not reaches. So this is expected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea9457d",
   "metadata": {},
   "source": [
    "## 3.3 Write file and add to NAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60a3049",
   "metadata": {},
   "outputs": [],
   "source": [
    "SFR_data.reach_data = DF_reach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e92b865",
   "metadata": {},
   "outputs": [],
   "source": [
    "SFR_data.write_package(d_Pa['SFR'], version='mf6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb181299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to find an inteernal SFRmaker way to fix this later. This is just a temporary patch.\n",
    "with open(d_Pa['SFR'], 'r+', encoding='cp1252') as f:\n",
    "    content = f.read()\n",
    "    content = content.replace(f\"FILEIN {MdlN}.SFR6.obs\", f\"FILEIN imported_model/{MdlN}.SFR6.obs\")\n",
    "    content = content.replace('BUDGET FILEOUT', '#BUDGET FILEOUT')\n",
    "    f.seek(0)\n",
    "    f.truncate()\n",
    "    f.write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853562fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.rename('model_SFR.chk', PJ(d_Pa['MF6'], 'imported_model/model_SFR.chk'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9334cfb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(d_Pa['NAM_Mdl'], 'r') as f1:\n",
    "    l_Lns_NAM = f1.readlines()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff4484b",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_Lns_NAM.insert(-1, f\"  sfr6 imported_model/{PBN(d_Pa['SFR'])} sfr\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfb9b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(d_Pa['NAM_Mdl'], 'w') as f2:\n",
    "    f2.writelines(l_Lns_NAM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc0a4b8",
   "metadata": {},
   "source": [
    "# 4. Connect DRN to SFR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88d8203",
   "metadata": {},
   "source": [
    "## 4.1 Attempt 1 - load DRN from previous Sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d8a7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# d_DRN_Pa = [r'C:\\OD\\WS_Mdl\\models\\NBr\\Sim\\NBr18\\GWF_1\\MODELINPUT\\DRN6\\SYS1\\DRN_T1.ARR',\n",
    "#             r'C:\\OD\\WS_Mdl\\models\\NBr\\Sim\\NBr18\\GWF_1\\MODELINPUT\\DRN6\\SYS3\\DRN_T1.ARR',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e058a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_DRN(A_DRN):\n",
    "#     # Plot A_DRN elevation and conductance\n",
    "#     fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "#     # Plot elevation\n",
    "#     A_DRN['elevation'].plot(ax=axes[0], cmap='viridis', \n",
    "#                             cbar_kwargs={'label': 'Elevation (m)'})\n",
    "#     axes[0].set_title('A_DRN Elevation')\n",
    "#     axes[0].set_aspect('equal')\n",
    "\n",
    "#     # Plot conductance\n",
    "#     A_DRN['conductance'].plot(ax=axes[1], cmap='plasma',\n",
    "#                             cbar_kwargs={'label': 'Conductance (1/d)'})\n",
    "#     axes[1].set_title('A_DRN Conductance')\n",
    "#     axes[1].set_aspect('equal')\n",
    "\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "#     # Print some basic statistics\n",
    "#     print(f\"\\nA_DRN Elevation stats:\")\n",
    "#     print(f\"Min: {A_DRN['elevation'].min().values:.2f}\")\n",
    "#     print(f\"Max: {A_DRN['elevation'].max().values:.2f}\")\n",
    "#     print(f\"Valid values: {(~np.isnan(A_DRN['elevation'])).sum().values}\")\n",
    "\n",
    "#     print(f\"\\nA_DRN Conductance stats:\")\n",
    "#     print(f\"Min: {A_DRN['conductance'].min().values:.2f}\")\n",
    "#     print(f\"Max: {A_DRN['conductance'].max().values:.2f}\")\n",
    "#     print(f\"Valid values: {(~np.isnan(A_DRN['conductance'])).sum().values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4394c71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# d_A_DRN = {i: MF6_Mdl.data[i].dataset.isel(layer=0, time=0).sel() for i in MF6_Mdl.data.keys() if 'drn' in i.lower()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9c1612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in d_A_DRN.keys():\n",
    "#     print(f\"\\n--- DRN Package: {i} ---\")\n",
    "#     plot_DRN(d_A_DRN[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f7e1a6",
   "metadata": {},
   "source": [
    "## 4.2 Attempt 2 - load DRN from current Sim\n",
    "This is much better, as we can directly connect the items...\n",
    "\n",
    "if it works... AI just mushed up some shit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ce5aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base = PJ(d_Pa['Pa_MdlN'], 'modflow6/imported_model')\n",
    "# folders = [f for f in os.listdir(base) if ('drn' in f.lower()) and not ('.' in f) and os.path.isdir(PJ(base, f))]\n",
    "# d_DRN_Pa = [PJ(base, folder, fname)\n",
    "#              for folder in folders\n",
    "#              for fname in os.listdir(PJ(base, folder))\n",
    "#              if os.path.isfile(PJ(base, folder, fname))]\n",
    "\n",
    "# d_DRN_Pa  # list of full paths to files inside the matched \"drn\" folders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63694702",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56374d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Read binary files from d_DRN_Pa to d_DRN_Bin\n",
    "# # Keys should be the number following drn-\n",
    "# import numpy as np\n",
    "# import struct\n",
    "\n",
    "# d_DRN_Bin = {}\n",
    "\n",
    "# for file_path in d_DRN_Pa:\n",
    "#     # Extract filename from path\n",
    "#     filename = os.path.basename(file_path)\n",
    "#     folder_name = os.path.basename(os.path.dirname(file_path))\n",
    "    \n",
    "#     print(f\"Processing: {filename} from folder: {folder_name}\")\n",
    "    \n",
    "#     # Extract key from folder name (drn-1, drn-2, etc.)\n",
    "#     key_found = False\n",
    "    \n",
    "#     # Look for drn- pattern in folder name first\n",
    "#     if 'drn-' in folder_name.lower() or 'drn_' in folder_name.lower():\n",
    "#         separator = 'drn-' if 'drn-' in folder_name.lower() else 'drn_'\n",
    "#         parts = folder_name.lower().split(separator)\n",
    "#         if len(parts) > 1:\n",
    "#             number_part = parts[1]\n",
    "            \n",
    "#             # Extract only the numeric part\n",
    "#             import re\n",
    "#             match = re.match(r'^(\\d+)', number_part)\n",
    "#             if match:\n",
    "#                 key = int(match.group(1))\n",
    "#                 key_found = True\n",
    "    \n",
    "#     # If not found in folder, try filename\n",
    "#     if not key_found:\n",
    "#         for name_source in [filename.lower()]:\n",
    "#             if 'drn-' in name_source or 'drn_' in name_source:\n",
    "#                 separator = 'drn-' if 'drn-' in name_source else 'drn_'\n",
    "#                 parts = name_source.split(separator)\n",
    "#                 if len(parts) > 1:\n",
    "#                     number_part = parts[1].split('.')[0]  # Remove file extension\n",
    "                    \n",
    "#                     import re\n",
    "#                     match = re.match(r'^(\\d+)', number_part)\n",
    "#                     if match:\n",
    "#                         key = int(match.group(1))\n",
    "#                         key_found = True\n",
    "#                         break\n",
    "    \n",
    "#     if not key_found:\n",
    "#         print(f\"  Could not extract key from {filename} or {folder_name}\")\n",
    "#         continue\n",
    "    \n",
    "#     try:\n",
    "#         # Read the binary file\n",
    "#         with open(file_path, 'rb') as f:\n",
    "#             data = f.read()\n",
    "            \n",
    "#         # Store the data with metadata\n",
    "#         d_DRN_Bin[key] = {\n",
    "#             'file_path': file_path,\n",
    "#             'filename': filename,\n",
    "#             'folder': folder_name,\n",
    "#             'size': len(data),\n",
    "#             'raw_data': data\n",
    "#         }\n",
    "        \n",
    "#         print(f\"  Successfully loaded {filename} with key {key} (size: {len(data)} bytes)\")\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(f\"  Error reading {filename}: {e}\")\n",
    "\n",
    "# print(f\"\\nLoaded {len(d_DRN_Bin)} binary files\")\n",
    "# print(f\"Keys: {sorted(d_DRN_Bin.keys())}\")\n",
    "\n",
    "# # Display summary\n",
    "# for key in sorted(d_DRN_Bin.keys()):\n",
    "#     info = d_DRN_Bin[key]\n",
    "#     print(f\"Key {key}: {info['filename']} from {info['folder']} ({info['size']} bytes)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e51e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Helper function to access binary data\n",
    "# def get_drn_binary_data(key):\n",
    "#     \"\"\"Get the raw binary data for a specific DRN key\"\"\"\n",
    "#     if key in d_DRN_Bin:\n",
    "#         return d_DRN_Bin[key]['raw_data']\n",
    "#     else:\n",
    "#         raise KeyError(f\"Key {key} not found in d_DRN_Bin. Available keys: {list(d_DRN_Bin.keys())}\")\n",
    "\n",
    "# # Example usage:\n",
    "# print(\"Available DRN binary files:\")\n",
    "# for key in sorted(d_DRN_Bin.keys()):\n",
    "#     info = d_DRN_Bin[key]\n",
    "#     print(f\"  Key {key}: {info['filename']} ({info['size']:,} bytes)\")\n",
    "    \n",
    "# print(f\"\\nExample: To access binary data for key 1: get_drn_binary_data(1)\")\n",
    "# print(f\"Example: Direct access to metadata: d_DRN_Bin[1]['file_path']\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6eb9b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # These are likely MODFLOW6 binary budget files - use flopy's correct method\n",
    "# import flopy.utils as fpu\n",
    "\n",
    "# d_DRN_Readable = {}\n",
    "\n",
    "# for key in sorted(d_DRN_Bin.keys()):\n",
    "#     info = d_DRN_Bin[key]\n",
    "#     file_path = info['file_path']\n",
    "    \n",
    "#     try:\n",
    "#         # Try as MODFLOW6 binary budget file\n",
    "#         budget = fpu.CellBudgetFile(file_path)\n",
    "#         records = budget.get_unique_record_names()\n",
    "#         times = budget.get_times()\n",
    "        \n",
    "#         d_DRN_Readable[key] = {\n",
    "#             'filename': info['filename'],\n",
    "#             'folder': info['folder'],\n",
    "#             'budget_file': budget,\n",
    "#             'records': records,\n",
    "#             'times': times,\n",
    "#             'summary': f\"DRN-{key}: Budget file with {len(records)} records, {len(times)} time steps\"\n",
    "#         }\n",
    "        \n",
    "#         print(f\"Key {key}: {d_DRN_Readable[key]['summary']}\")\n",
    "#         print(f\"  Records: {records}\")\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(f\"Key {key}: Failed - {e}\")\n",
    "\n",
    "# print(f\"\\nSuccessfully read {len(d_DRN_Readable)} DRN budget files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a37d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check what we actually have\n",
    "# print(\"d_DRN_Bin keys:\", list(d_DRN_Bin.keys()) if 'd_DRN_Bin' in globals() else \"Not found\")\n",
    "# print(\"d_DRN_Readable keys:\", list(d_DRN_Readable.keys()) if 'd_DRN_Readable' in globals() else \"Not found\")\n",
    "# print(\"d_DRN_Readable contents:\", d_DRN_Readable if 'd_DRN_Readable' in globals() else \"Not found\")\n",
    "\n",
    "# # If empty, let's create a simple readable version\n",
    "# if not d_DRN_Readable and 'd_DRN_Bin' in globals():\n",
    "#     print(\"\\\\nCreating simple readable format...\")\n",
    "#     d_DRN_Readable = {}\n",
    "#     for key in d_DRN_Bin.keys():\n",
    "#         d_DRN_Readable[key] = f\"DRN file {key}: {d_DRN_Bin[key]['filename']} ({d_DRN_Bin[key]['size']/1024/1024:.1f} MB)\"\n",
    "    \n",
    "#     print(\"Success! d_DRN_Readable now contains:\", d_DRN_Readable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58dcba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Accessing File Contents\n",
    "\n",
    "# #You can access the binary file contents in several ways:\n",
    "\n",
    "# # How to access file contents from d_DRN_Bin\n",
    "\n",
    "# # 1. Access raw binary data directly\n",
    "# print(\"Raw binary data for file 1 (first 100 bytes):\")\n",
    "# print(d_DRN_Bin[1]['raw_data'][:100])\n",
    "\n",
    "# # 2. Get file information\n",
    "# print(\"\\nFile information:\")\n",
    "# for key in d_DRN_Bin:\n",
    "#     info = d_DRN_Bin[key]\n",
    "#     print(f\"File {key}: {info['filename']} from {info['folder']}\")\n",
    "#     print(f\"  Full path: {info['file_path']}\")\n",
    "#     print(f\"  Size: {info['size']} bytes ({info['size']/1024/1024:.1f} MB)\")\n",
    "#     print(f\"  Raw data length: {len(info['raw_data'])} bytes\")\n",
    "#     print()\n",
    "\n",
    "# # 3. Access the actual binary content for processing\n",
    "# for key in [1, 2, 3]:\n",
    "#     raw_bytes = d_DRN_Bin[key]['raw_data']\n",
    "#     print(f\"File {key} - First 20 bytes as hex: {raw_bytes[:20].hex()}\")\n",
    "#     print(f\"File {key} - First 20 bytes as text (try): {raw_bytes[:20]}\")\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05219aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check the actual structure of d_DRN_Bin\n",
    "# print(\"Structure of d_DRN_Bin:\")\n",
    "# for key in d_DRN_Bin:\n",
    "#     print(f\"Key {key}: {type(d_DRN_Bin[key])}\")\n",
    "#     if isinstance(d_DRN_Bin[key], dict):\n",
    "#         print(f\"  Keys in dictionary: {list(d_DRN_Bin[key].keys())}\")\n",
    "#     else:\n",
    "#         print(f\"  Direct content type: {type(d_DRN_Bin[key])}\")\n",
    "#         print(f\"  Content length: {len(d_DRN_Bin[key])} bytes\")\n",
    "#         print(f\"  First 50 bytes: {d_DRN_Bin[key][:50]}\")\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dea8871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# d_DRN_Bin[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8fd4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(d_DRN_Readable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12951789",
   "metadata": {},
   "source": [
    "## 4.3 Attempt #3\n",
    "With some help from Chat GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5424b18e",
   "metadata": {},
   "source": [
    "### 4.3.1 Prepare DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315b707b",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = PJ(d_Pa['Pa_MdlN'], 'modflow6/imported_model')\n",
    "folders = [f for f in os.listdir(base) if ('drn' in f.lower()) and not ('.' in f) and os.path.isdir(PJ(base, f))]\n",
    "l_DRN_Pa = [PJ(base, folder, fname)\n",
    "             for folder in folders\n",
    "             for fname in os.listdir(PJ(base, folder))\n",
    "             if os.path.isfile(PJ(base, folder, fname))]\n",
    "# l_DRN_Pa  # list of full paths to files inside the matched \"drn\" folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9f872e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_mf6_drn_bin(filepath: str | Path) -> pd.DataFrame:\n",
    "    \"\"\"Read MODFLOW 6 DRN binary input (imod format) into a DataFrame.\"\"\"\n",
    "    dtype = np.dtype([\n",
    "        (\"k\",    \"<i4\"),   # layer\n",
    "        (\"i\",    \"<i4\"),   # row\n",
    "        (\"j\",    \"<i4\"),   # column\n",
    "        (\"elev\", \"<f8\"),   # elevation\n",
    "        (\"cond\", \"<f8\"),   # conductance\n",
    "    ])\n",
    "    path = Path(filepath)\n",
    "    nrec = path.stat().st_size // dtype.itemsize\n",
    "    arr = np.fromfile(path, dtype=dtype, count=nrec)\n",
    "    return pd.DataFrame(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541ec348",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_DRN_DF = {}\n",
    "\n",
    "for i in range(len(l_DRN_Pa)):\n",
    "    DF_DRN = read_mf6_drn_bin(l_DRN_Pa[i])\n",
    "    d_DRN_DF[int(re.search(r'(?i)drn[-_]?(\\d+)', PDN(l_DRN_Pa[i])).group(1))] = DF_DRN.loc[ ~DF_DRN['i'].isin([1, N_R]) & ~DF_DRN['j'].isin([1, N_C]) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341a39da",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in d_DRN_DF.keys():\n",
    "    # print(f\"DRN-{k} DataFrame shape: {d_DRN_DF[k].shape}\")\n",
    "    d_DRN_DF[k] = U.Calc_DF_XY(d_DRN_DF[k], X_Ogn, Y_Ogn, cellsize)\n",
    "    d_DRN_DF[k].drop(columns=['cond', 'elev'], inplace=True)\n",
    "    d_DRN_DF[k]['Pkg1'] = f'drn-{k}'\n",
    "    d_DRN_DF[k]['Pvd_ID'] = d_DRN_DF[k].index + 1  # 1-based index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d5a5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_reach_for_DRN = U.Calc_DF_XY(DF_reach[['rno', 'i', 'j']], X_Ogn, Y_Ogn, cellsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0645356",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all DRN DataFrames and match with reach points by minimum distance\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "# Combine all d_DRN_DF items into a single DataFrame  \n",
    "DF_DRN_all = pd.concat(d_DRN_DF.values(), ignore_index=True)\n",
    "\n",
    "# Calculate distances and find closest reach for each DRN point\n",
    "drn_coords = DF_DRN_all[['X', 'Y']].values\n",
    "reach_coords = DF_reach_for_DRN[['X', 'Y']].values\n",
    "distances = cdist(drn_coords, reach_coords, metric='euclidean')\n",
    "min_indices = np.argmin(distances, axis=1)\n",
    "\n",
    "# Add matched reach data to DRN DataFrame\n",
    "matched_reach_data = DF_reach_for_DRN.iloc[min_indices].reset_index(drop=True)\n",
    "DF_DRN_all_matched = DF_DRN_all.copy()\n",
    "DF_DRN_all_matched['Rcv_ID'] = matched_reach_data['rno'].values\n",
    "DF_DRN_all_matched['distance_to_match'] = distances[np.arange(len(drn_coords)), min_indices]\n",
    "\n",
    "print(f\"Combined {len(DF_DRN_all):,} DRN points from {len(d_DRN_DF)} DataFrames\")\n",
    "print(f\"Matched to {DF_DRN_all_matched['Rcv_ID'].nunique()} unique reaches\")\n",
    "print(f\"Mean distance: {DF_DRN_all_matched['distance_to_match'].mean():.0f}m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641fb094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Quick summary of matching results\n",
    "# print(f\"Results: {len(DF_DRN_all_matched):,} DRN points matched\")\n",
    "# print(f\"Distance stats: mean={DF_DRN_all_matched['distance_to_match'].mean():.0f}m, \"\n",
    "#       f\"perfect_matches={(DF_DRN_all_matched['distance_to_match'] == 0).sum():,}\")\n",
    "# print(DF_DRN_all_matched[['k', 'i', 'j', 'X', 'Y', 'Pkg1', 'Rcv_ID', 'distance_to_match']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59eeb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_DRN_all_matched['Pkd2'] = 'sfr'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d072b388",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_DRN_write = DF_DRN_all_matched[['Pkg1', 'Pvd_ID', 'Pkd2', 'Rcv_ID']]\n",
    "DF_DRN_write['MVR_TYPE'] = 'FACTOR'\n",
    "DF_DRN_write['value'] = 1\n",
    "DF_DRN_write"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379c06fc",
   "metadata": {},
   "source": [
    "### 4.3.2 Write MVR file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81bc74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Pa_MVR = PJ(d_Pa['Sim_In'], f'{MdlN}.MVR6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab378f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(Pa_MVR, 'w') as f:\n",
    "    f.write(f\"\"\"BEGIN OPTIONS\n",
    "END OPTIONS\n",
    "\n",
    "BEGIN DIMENSIONS\n",
    "    MAXMVR {DF_DRN_write.shape[0]}\n",
    "    MAXPACKAGES {len(d_DRN_DF.keys())+1}\n",
    "END DIMENSIONS\n",
    "\n",
    "BEGIN PACKAGES\n",
    "    {'\\n    '.join([f\"drn-{k}\" for  k in d_DRN_DF.keys()])}\n",
    "    sfr\n",
    "END PACKAGES\n",
    "\n",
    "BEGIN PERIOD 1\n",
    "\"\"\")\n",
    "    f.write(U.DF_to_MF_block(DF_DRN_write))\n",
    "    f.write('END PERIOD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36bdd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert MVR line to NAM\n",
    "with open(d_Pa['NAM_Mdl'], 'r') as f1:\n",
    "    l_Lns_NAM = f1.readlines()  \n",
    "\n",
    "l_Lns_NAM.insert(-1, f\"  MVR6 imported_model/{PBN(Pa_MVR)} MVR\\n\")\n",
    "\n",
    "with open(d_Pa['NAM_Mdl'], 'w') as f2:\n",
    "    f2.writelines(l_Lns_NAM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76efa51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add MOVER option to SFR\n",
    "with open(d_Pa['SFR'], 'r') as f1:\n",
    "    l_Lns_SFR = f1.readlines()  \n",
    "\n",
    "l_Lns_SFR.insert(3, f\"  MOVER\\n\")\n",
    "\n",
    "with open(d_Pa['SFR'], 'w') as f2:\n",
    "    f2.writelines(l_Lns_SFR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945e06ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add MOVER option to DRN files\n",
    "for i in d_DRN_DF.keys():\n",
    "    with open(PJ(d_Pa['Sim_In'], f'drn-{i}.drn'), 'r') as f1:\n",
    "        l_Lns_DRN = f1.readlines()  \n",
    "\n",
    "    l_Lns_DRN.insert(3, f\"  MOVER\\n\")\n",
    "\n",
    "    with open(PJ(d_Pa['Sim_In'], f'drn-{i}.drn'), 'w') as f2:\n",
    "        f2.writelines(l_Lns_DRN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcb469b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b00359cd",
   "metadata": {},
   "source": [
    "# 5. Execute model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23b3f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(d_Pa['TOML'], 'a') as f: # Add enable_sprinkling = true to the end of the TOML file\n",
    "#     f.write('enable_sprinkling = true\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872869d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "U.mete_grid_add_missing_Cols(PJ(d_Pa['Pa_MdlN'], 'metaswap/mete_grid.inp'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdc0527",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990f769e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the coupled model\n",
    "print(\"ðŸš€ Starting coupled model execution...\")\n",
    "print(f\"Model directory: {d_Pa['Pa_MdlN']}\")\n",
    "\n",
    "# Check what files were written\n",
    "print(\"\\nðŸ“ Checking written model files:\")\n",
    "if PE(d_Pa['Pa_MdlN']):\n",
    "    model_files = LD(d_Pa['Pa_MdlN'])\n",
    "    for file in sorted(model_files):\n",
    "        print(f\"  - {file}\")\n",
    "    \n",
    "    # Look for the main execution file (usually .toml or similar)\n",
    "    toml_files = [f for f in model_files if f.endswith('.toml')]\n",
    "    if toml_files:\n",
    "        print(f\"\\nðŸŽ¯ Found TOML configuration file: {toml_files[0]}\")\n",
    "        main_toml = PJ(d_Pa['Pa_MdlN'], toml_files[0])\n",
    "        \n",
    "        # Since we have the DLL paths, we can try to execute using the iMOD coupler\n",
    "        # The iMOD coupler typically needs the .toml file as input\n",
    "        coupler_exe = Pa_IMC\n",
    "        \n",
    "        if PE(coupler_exe):\n",
    "            print(f\"âœ… Found iMOD coupler: {coupler_exe}\")\n",
    "            print(f\"ðŸ”„ Executing: {coupler_exe} {main_toml}\")\n",
    "            \n",
    "            # Execute the model (this will take some time)\n",
    "            import subprocess\n",
    "            try:\n",
    "                result = subprocess.run([coupler_exe, main_toml], \n",
    "                                      cwd=d_Pa['Pa_MdlN'], \n",
    "                                      capture_output=True, \n",
    "                                      text=True,\n",
    "                                      timeout=3600)  # 1 hour timeout\n",
    "                \n",
    "                print(f\"Return code: {result.returncode}\")\n",
    "                if result.stdout:\n",
    "                    print(\"STDOUT:\")\n",
    "                    print(result.stdout)\n",
    "                if result.stderr:\n",
    "                    print(\"STDERR:\")\n",
    "                    print(result.stderr)\n",
    "                    \n",
    "                if result.returncode == 0:\n",
    "                    print(\"âœ… Model execution completed successfully!\")\n",
    "                else:\n",
    "                    print(f\"âŒ Model execution failed with return code {result.returncode}\")\n",
    "                    \n",
    "            except subprocess.TimeoutExpired:\n",
    "                print(\"â° Model execution timed out after 1 hour\")\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ Error executing model: {e}\")\n",
    "        else:\n",
    "            print(f\"âŒ iMOD coupler not found at: {coupler_exe}\")\n",
    "            print(\"You may need to execute the model manually using the iMOD coupler\")\n",
    "    else:\n",
    "        print(\"âŒ No TOML configuration file found\")\n",
    "else:\n",
    "    print(f\"âŒ Model directory not found: {d_Pa['Pa_MdlN']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1364f2e",
   "metadata": {},
   "source": [
    "Model executed sucessfully!!! More improvements next Sim (NBr35)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ee8110",
   "metadata": {},
   "source": [
    "# -1. Junkyard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539bae68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create flopy MF6 model with DIS package from existing MF6_DIS\n",
    "# def create_flopy_model_from_imod_dis(mf6_dis, model_name=\"gwf_model\"):\n",
    "#     \"\"\"\n",
    "#     Convert imod MF6_DIS to flopy MF6 model with DIS package\n",
    "#     \"\"\"\n",
    "#     # Extract data from imod DIS\n",
    "#     dataset = mf6_dis.dataset\n",
    "#     nlay, nrow, ncol = dataset.dims['layer'], dataset.dims['y'], dataset.dims['x']\n",
    "#     dx = abs(float(dataset.coords['dx'].values))\n",
    "#     dy = abs(float(dataset.coords['dy'].values))\n",
    "    \n",
    "#     # Get elevation arrays\n",
    "#     top_array = dataset['top'].values\n",
    "#     bottom_array = dataset['bottom'].values\n",
    "#     idomain_array = dataset['idomain'].values\n",
    "    \n",
    "#     # Calculate grid origin \n",
    "#     x_coords = dataset.coords['x'].values\n",
    "#     y_coords = dataset.coords['y'].values\n",
    "#     xorigin = x_coords[0] - dx/2\n",
    "#     yorigin = y_coords[-1] - dy/2\n",
    "    \n",
    "#     # Create flopy simulation\n",
    "#     model_ws = \"flopy_model\"\n",
    "#     os.makedirs(model_ws, exist_ok=True)\n",
    "    \n",
    "#     sim = mf6.MFSimulation(sim_name=\"mf6_sim\", sim_ws=model_ws, exe_name='mf6')\n",
    "#     tdis = mf6.ModflowTdis(sim, nper=1, perioddata=[(1.0, 1, 1.0)])\n",
    "#     ims = mf6.ModflowIms(sim, print_option='ALL')\n",
    "    \n",
    "#     # Create groundwater flow model\n",
    "#     gwf = mf6.MFModel(simulation=sim, model_type='gwf6', modelname=model_name)\n",
    "    \n",
    "#     # Create DIS package\n",
    "#     dis = mf6.ModflowGwfdis(\n",
    "#         gwf,\n",
    "#         nlay=nlay, nrow=nrow, ncol=ncol,\n",
    "#         delr=dx, delc=dy,\n",
    "#         top=top_array,\n",
    "#         botm=bottom_array,\n",
    "#         idomain=idomain_array,\n",
    "#         xorigin=xorigin,\n",
    "#         yorigin=yorigin\n",
    "#     )\n",
    "    \n",
    "#     print(f\"âœ“ Created flopy MF6 model with DIS:\")\n",
    "#     print(f\"  - Grid: {nlay} layers, {nrow} rows, {ncol} cols\")\n",
    "#     print(f\"  - Cell size: {dx} x {dy} m\")\n",
    "#     print(f\"  - Origin: ({xorigin:.0f}, {yorigin:.0f})\")\n",
    "#     print(f\"  - Workspace: {model_ws}\")\n",
    "    \n",
    "#     return sim, gwf, dis\n",
    "\n",
    "# # Create flopy model\n",
    "# sim, gwf, dis = create_flopy_model_from_imod_dis(MF6_DIS)\n",
    "\n",
    "# # Access the flopy grid object\n",
    "# flopy_grid = gwf.modelgrid\n",
    "# print(f\"\\\\nâœ“ Flopy StructuredGrid created:\")\n",
    "# print(f\"  - Shape: {flopy_grid.shape}\")\n",
    "# print(f\"  - Extent: {flopy_grid.extent}\")\n",
    "# print(f\"  - Grid ready for use with flopy tools!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947aff9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create proper metadata dictionary for all 37 layers\n",
    "# n_layers = DS.bottom.shape[0]  # Should be 37\n",
    "# d_MtDt_bottom = {}\n",
    "\n",
    "# # Add metadata for each layer\n",
    "# for i in range(n_layers):\n",
    "#     layer_num = i + 1  # 1-based layer numbering\n",
    "#     d_MtDt_bottom[f'layer_{layer_num:02d}'] = {\n",
    "#         'layer': f'L{layer_num}',\n",
    "#         'description': f'Bottom elevation for layer {layer_num}',\n",
    "#         'units': 'm'\n",
    "#     }\n",
    "\n",
    "# # Add global metadata\n",
    "# d_MtDt_bottom['all'] = {\n",
    "#     'metadata': f\"Created on {DT.today().strftime('%Y-%m-%d %H:%M:%S')}, to sense check SFRmaker's calculated Ls against the real grid. 100x100 gets interpolated.\",\n",
    "#     'source': 'DS.bottom',\n",
    "#     'total_layers': n_layers\n",
    "# }\n",
    "\n",
    "# print(f\"Creating TIF with {n_layers} bands...\")\n",
    "# print(f\"Metadata keys: {len([k for k in d_MtDt_bottom.keys() if k != 'all'])} bands\")\n",
    "\n",
    "# # Now call the function with proper metadata\n",
    "# G.DA_to_MBTIF(\n",
    "#     DS.bottom, \n",
    "#     r\"C:\\OD\\WS_Mdl\\models\\NBr\\PoP\\In\\BOT\\NBr1\\BOT_NBr1_25x25.tif\", \n",
    "#     d_MtDt_bottom, \n",
    "#     crs=G.crs, \n",
    "#     _print=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3548f3",
   "metadata": {},
   "source": [
    "##### k=0 investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e615850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"SFR Grid dimensions: {SFR_grid.nrow} rows Ã— {SFR_grid.ncol} cols\")\n",
    "# print(f\"Max node in reach data: {DF_reach['node'].max()}\")\n",
    "# print(f\"Expected max node for 2D grid: {SFR_grid.nrow * SFR_grid.ncol - 1}\")\n",
    "# print(f\"Max i in reach data: {DF_reach['i'].max()}\")\n",
    "# print(f\"Max j in reach data: {DF_reach['j'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0e032f",
   "metadata": {},
   "source": [
    "**Why is SFRmaker doing this incorrect conversion?**\n",
    "\n",
    "The issue is in SFRmaker's source code (`sfrmaker/lines.py` around lines 356-372). \n",
    "\n",
    "**The Problem:**\n",
    "- SFRmaker hardcodes `k = 0` for all reaches \n",
    "- It assumes all node numbers are **2D nodes** (layer 0 only)\n",
    "- It uses 2D conversion formulas: `i = node Ã· ncol` and `j = node % ncol`\n",
    "\n",
    "**What's happening in your case:**\n",
    "- Your model has **3D nodes** that include layer information\n",
    "- SFRmaker receives 3D node numbers (like 1,155,680) \n",
    "- It incorrectly applies 2D conversion to these 3D nodes\n",
    "- This gives wrong `i` values (like 2,407 instead of proper row indices 0-343)\n",
    "\n",
    "**The correct 3D conversion should be:**\n",
    "- `k = node Ã· (nrow Ã— ncol)`\n",
    "- `i = (node % (nrow Ã— ncol)) Ã· ncol` \n",
    "- `j = node % ncol`\n",
    "\n",
    "This is a bug in SFRmaker - it doesn't handle 3D structured grids properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d615b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Let's demonstrate the problem with a few example nodes from your data\n",
    "# import numpy as np\n",
    "\n",
    "# # Get some example 3D nodes from your reach data\n",
    "# example_nodes = DF_reach['node'].iloc[::10000].tolist()[:5]  # Every 10000th node, first 5\n",
    "# print(\"Example 3D nodes from your data:\", example_nodes)\n",
    "# print()\n",
    "\n",
    "# # Grid dimensions\n",
    "# nrow, ncol = SFR_grid.nrow, SFR_grid.ncol  # 344, 480\n",
    "# print(f\"Grid: {nrow} rows Ã— {ncol} cols\")\n",
    "# print()\n",
    "\n",
    "# # Show what SFRmaker is doing (INCORRECT for 3D nodes)\n",
    "# print(\"âŒ SFRmaker's INCORRECT 2D conversion:\")\n",
    "# for node in example_nodes:\n",
    "#     i_wrong = node // ncol  # SFRmaker's approach\n",
    "#     j = node % ncol\n",
    "#     k = 0  # SFRmaker hardcodes this\n",
    "#     print(f\"  Node {node:>7} â†’ k={k}, i={i_wrong:>4}, j={j:>3}\")\n",
    "# print()\n",
    "\n",
    "# # Show what the CORRECT 3D conversion should be\n",
    "# print(\"âœ… CORRECT 3D conversion:\")\n",
    "# for node in example_nodes:\n",
    "#     k_correct = node // (nrow * ncol)\n",
    "#     i_correct = (node % (nrow * ncol)) // ncol\n",
    "#     j_correct = node % ncol\n",
    "#     print(f\"  Node {node:>7} â†’ k={k_correct}, i={i_correct:>3}, j={j_correct:>3}\")\n",
    "    \n",
    "# print(f\"\\nðŸ“Š Your data statistics:\")\n",
    "# print(f\"   Max i in data: {DF_reach['i'].max()} (should be â‰¤ {nrow-1})\")\n",
    "# print(f\"   Max j in data: {DF_reach['j'].max()} (should be â‰¤ {ncol-1}) âœ“\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b10196",
   "metadata": {},
   "source": [
    "##### testing relics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d811c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#N_L, N_R, N_C, f\"{N_L * N_R * N_C:,}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91589fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DF_reach['node'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12491933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DF_reach['i_copy'] = DF_reach['i'].copy()\n",
    "# DF_reach['k_copy'] = DF_reach['k'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8918b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DF_reach['test'] = (DF_reach['i_copy'] % (DF_reach['k'] * DF_reach['j']))\n",
    "# DF_reach[['node', 'k', 'i', 'j', 'test']].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd31d2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DF_test = DF_reach[['node', 'k', 'i_copy', 'j']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02613226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DF_test['k_a'] = DF_reach['node'] // (N_R * N_C)\n",
    "# DF_test['k_b'] = DF_reach['i_copy'] % (N_R *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2edfbfef",
   "metadata": {},
   "source": [
    "#### Co-pilot assign layer error investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98bed559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Let's look for the exact case from the previous output\n",
    "# # From cell 173 output, we saw k=6, i=281, j=392 with strtop=19.759\n",
    "# test_case = DF_reach[(DF_reach['strtop'] > 19.75) & (DF_reach['strtop'] < 19.77)]\n",
    "# print(\"Found test cases with strtop around 19.76:\")\n",
    "# print(test_case[['k', 'i', 'j', 'strtop', 'strthick', 'rno']].head())\n",
    "\n",
    "# if len(test_case) > 0:\n",
    "#     # Take the first match\n",
    "#     k, i, j, strtop, strthick = test_case.iloc[0][['k', 'i', 'j', 'strtop', 'strthick']]\n",
    "#     strbot_calculated = strtop - strthick\n",
    "#     print(f\"\\nAnalyzing case: k={k}, i={i}, j={j}\")\n",
    "#     print(f\"strtop: {strtop}\")\n",
    "#     print(f\"strthick: {strthick}\")\n",
    "#     print(f\"strbot (strtop - strthick): {strbot_calculated}\")\n",
    "    \n",
    "#     # Get the BOTs at this location (using 0-based indexing)\n",
    "#     print(f\"\\nLayer bottoms at i={i}, j={j}:\")\n",
    "#     for L in range(min(10, BOTs.shape[0])):  # Show first 10 layers\n",
    "#         bot = BOTs[L, i, j]\n",
    "#         print(f\"L{L+1} bottom: {bot:.6f}\")\n",
    "        \n",
    "#     # Let's check what layer the reach was actually assigned to\n",
    "#     assigned_k = reach_Ls[test_case.index[0]]\n",
    "#     print(f\"\\nReach was assigned to layer k={assigned_k+1} (1-based) = k={assigned_k} (0-based)\")\n",
    "    \n",
    "#     # Check where strbot should be assigned\n",
    "#     print(f\"\\nAnalyzing where strbot ({strbot_calculated:.6f}) should be assigned:\")\n",
    "#     for L in range(min(8, BOTs.shape[0]-1)):  # Check first 8 layers\n",
    "#         if L == 0:\n",
    "#             top = TOPs[0, i, j]  # First layer top - TOPs is 3D like BOTs\n",
    "#         else:\n",
    "#             top = BOTs[L-1, i, j]  # Previous layer bottom = current layer top\n",
    "#         bot = BOTs[L, i, j]\n",
    "#         fits = bot <= strbot_calculated <= top\n",
    "#         print(f\"L{L+1}: top={top:.6f}, bot={bot:.6f} - strbot fits? {fits}\")\n",
    "#         if fits:\n",
    "#             print(f\"  --> Stream bottom SHOULD be in Layer {L+1} (0-based: {L})\")\n",
    "#             if L != assigned_k:\n",
    "#                 print(f\"  *** PROBLEM: But it was assigned to Layer {assigned_k+1} (0-based: {assigned_k}) ***\")\n",
    "#             break\n",
    "    \n",
    "#     # Let's also test what happens if we use strthick=1.0 instead of 0.1\n",
    "#     strbot_with_thick_1 = strtop - 1.0\n",
    "#     print(f\"\\nIf strthick was 1.0 instead of 0.1:\")\n",
    "#     print(f\"strbot would be: {strbot_with_thick_1:.6f}\")\n",
    "#     for L in range(min(8, BOTs.shape[0]-1)):\n",
    "#         if L == 0:\n",
    "#             top = TOPs[0, i, j]\n",
    "#         else:\n",
    "#             top = BOTs[L-1, i, j]\n",
    "#         bot = BOTs[L, i, j]\n",
    "#         fits = bot <= strbot_with_thick_1 <= top\n",
    "#         if fits:\n",
    "#             print(f\"  --> With strthick=1.0, would be assigned to Layer {L+1} (0-based: {L})\")\n",
    "#             if L == assigned_k:\n",
    "#                 print(f\"  *** This matches the actual assignment! The function may be ignoring strthick=0.1 ***\")\n",
    "#             break\n",
    "# else:\n",
    "#     print(\"No test case found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08bd0106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Let's test the assign_layers function more systematically\n",
    "# # Create a small test case to verify if it's using strthick properly\n",
    "\n",
    "# print(\"=== TESTING sfr.utils.assign_layers BEHAVIOR ===\")\n",
    "# print()\n",
    "\n",
    "# # Create a minimal test dataframe with known values\n",
    "# test_df = DF_reach.iloc[3910:3912].copy()  # Take the problematic cases\n",
    "# print(\"Test data:\")\n",
    "# print(test_df[['k', 'i', 'j', 'strtop', 'strthick']])\n",
    "# print()\n",
    "\n",
    "# # Test 1: Call with strthick = 0.1 (current values)\n",
    "# print(\"Test 1: Using strthick = 0.1 (current values)\")\n",
    "# test_layers_01, _ = sfr.utils.assign_layers(reach_data=test_df, botm_array=BOTs)\n",
    "# print(\"Assigned layers:\", test_layers_01)\n",
    "# print()\n",
    "\n",
    "# # Test 2: Temporarily change strthick to 1.0 and see what happens\n",
    "# test_df_thick1 = test_df.copy()\n",
    "# test_df_thick1['strthick'] = 1.0\n",
    "# print(\"Test 2: Changing strthick to 1.0\")\n",
    "# print(test_df_thick1[['k', 'i', 'j', 'strtop', 'strthick']])\n",
    "# test_layers_10, _ = sfr.utils.assign_layers(reach_data=test_df_thick1, botm_array=BOTs)\n",
    "# print(\"Assigned layers:\", test_layers_10)\n",
    "# print()\n",
    "\n",
    "# # Test 3: What if we use a different column name?\n",
    "# test_df_custom = test_df.copy()\n",
    "# test_df_custom['custom_thick'] = 0.1\n",
    "# print(\"Test 3: Using custom column name 'custom_thick'\")\n",
    "# test_layers_custom, _ = sfr.utils.assign_layers(reach_data=test_df_custom, botm_array=BOTs, strthick_col='custom_thick')\n",
    "# print(\"Assigned layers:\", test_layers_custom)\n",
    "# print()\n",
    "\n",
    "# print(\"=== ANALYSIS ===\")\n",
    "# print(f\"With strthick=0.1: {test_layers_01}\")\n",
    "# print(f\"With strthick=1.0: {test_layers_10}\")\n",
    "# print(f\"With custom column: {test_layers_custom}\")\n",
    "\n",
    "# if np.array_equal(test_layers_01, test_layers_10):\n",
    "#     print(\"âŒ PROBLEM: strthick=0.1 and strthick=1.0 give SAME results!\")\n",
    "#     print(\"   This suggests the function is NOT using the strthick column.\")\n",
    "# elif np.array_equal(test_layers_01, test_layers_custom):\n",
    "#     print(\"âœ… The function DOES use the strthick column correctly.\")\n",
    "# else:\n",
    "#     print(\"ðŸ¤” Mixed results - need further investigation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6a66b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Let's test specifically with the problematic case we found earlier\n",
    "# print(\"=== TESTING THE SPECIFIC PROBLEMATIC CASE ===\")\n",
    "\n",
    "# # Get the exact case that was wrong\n",
    "# problematic_case = DF_reach[(DF_reach['strtop'] > 19.75) & (DF_reach['strtop'] < 19.77)].iloc[[0]]\n",
    "# print(\"Problematic case:\")\n",
    "# print(problematic_case[['k', 'i', 'j', 'strtop', 'strthick']])\n",
    "\n",
    "# # Test it individually\n",
    "# test_layer, _ = sfr.utils.assign_layers(reach_data=problematic_case, botm_array=BOTs)\n",
    "# print(f\"Function assigned layer: {test_layer} (0-based)\")\n",
    "# print(f\"As scalar: {test_layer if np.isscalar(test_layer) else test_layer[0]} (0-based)\")\n",
    "\n",
    "# # Let's manually calculate what it should be\n",
    "# k, i, j, strtop, strthick = problematic_case.iloc[0][['k', 'i', 'j', 'strtop', 'strthick']]\n",
    "# strbot = strtop - strthick\n",
    "\n",
    "# print(f\"\\nManual calculation:\")\n",
    "# print(f\"strtop: {strtop}\")\n",
    "# print(f\"strthick: {strthick}\")  \n",
    "# print(f\"strbot: {strbot}\")\n",
    "\n",
    "# # Find the correct layer manually\n",
    "# for L in range(BOTs.shape[0]-1):\n",
    "#     if L == 0:\n",
    "#         top = TOPs[0, i, j]\n",
    "#     else:\n",
    "#         top = BOTs[L-1, i, j]\n",
    "#     bot = BOTs[L, i, j]\n",
    "    \n",
    "#     if bot <= strbot <= top:\n",
    "#         print(f\"Manual calculation says layer should be: {L} (0-based)\")\n",
    "#         if L != (test_layer if np.isscalar(test_layer) else test_layer[0]):\n",
    "#             print(f\"âŒ MISMATCH: Function returned {test_layer}, should be {L}\")\n",
    "#         else:\n",
    "#             print(f\"âœ… MATCH: Function correctly returned {L}\")\n",
    "#         break\n",
    "\n",
    "# # Let's also check if there might be a pad parameter affecting this\n",
    "# print(f\"\\nLet's check what happens with different pad values:\")\n",
    "# for pad in [0.0, 1.0, 2.0]:\n",
    "#     test_layer_pad, _ = sfr.utils.assign_layers(reach_data=problematic_case, botm_array=BOTs, pad=pad)\n",
    "#     layer_val = test_layer_pad if np.isscalar(test_layer_pad) else test_layer_pad[0] \n",
    "#     print(f\"With pad={pad}: layer = {layer_val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7671cde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Let's explore the logic behind the 'pad' parameter\n",
    "# print(\"=== UNDERSTANDING THE 'pad' PARAMETER LOGIC ===\")\n",
    "\n",
    "# # From the documentation:\n",
    "# # \"Minimum distance that streambed bottom must be above layer bottom.\n",
    "# #  When determining the layer or whether the streambed bottom is below\n",
    "# #  the model bottom, streambed bottom - pad is used.\"\n",
    "\n",
    "# k, i, j, strtop, strthick = problematic_case.iloc[0][['k', 'i', 'j', 'strtop', 'strthick']]\n",
    "# strbot = strtop - strthick\n",
    "\n",
    "# print(f\"Example case: strtop={strtop:.3f}, strthick={strthick}, strbot={strbot:.3f}\")\n",
    "# print(f\"\\nLayer boundaries at this location:\")\n",
    "\n",
    "# for L in range(7):  # Show first 7 layers\n",
    "#     if L == 0:\n",
    "#         top = TOPs[0, i, j]\n",
    "#     else:\n",
    "#         top = BOTs[L-1, i, j]\n",
    "#     bot = BOTs[L, i, j]\n",
    "    \n",
    "#     print(f\"L{L+1}: top={top:.3f}, bot={bot:.3f}, thickness={top-bot:.3f}\")\n",
    "    \n",
    "#     # Show what happens with different pad values\n",
    "#     for pad in [0.0, 0.5, 1.0]:\n",
    "#         effective_strbot = strbot - pad\n",
    "#         fits = bot <= effective_strbot <= top\n",
    "#         if fits:\n",
    "#             print(f\"  â†’ With pad={pad}: effective_strbot={effective_strbot:.3f} fits in L{L+1}\")\n",
    "\n",
    "# print(f\"\\n=== WHY USE PAD? ===\")\n",
    "# print(f\"1. NUMERICAL STABILITY: Prevents streams from being placed exactly on layer boundaries\")\n",
    "# print(f\"2. HYDROGEOLOGICAL REALISM: Ensures minimum thickness between stream and impermeable layer\")\n",
    "# print(f\"3. MODEL CONVERGENCE: Avoids numerical issues when stream and layer bottom are too close\")\n",
    "# print(f\"4. SAFETY MARGIN: Accounts for elevation uncertainties in DEM/bathymetry data\")\n",
    "\n",
    "# print(f\"\\n=== PRACTICAL IMPLICATIONS ===\")\n",
    "# print(f\"â€¢ pad=0.0: Stream can be right at layer bottom (risky for convergence)\")\n",
    "# print(f\"â€¢ pad=1.0: Stream must be at least 1m above layer bottom (default, conservative)\")\n",
    "# print(f\"â€¢ pad=0.1-0.5: Reasonable compromise for most applications\")\n",
    "\n",
    "# print(f\"\\n=== WHEN MIGHT YOU WANT DIFFERENT pad VALUES? ===\")\n",
    "# print(f\"â€¢ High-resolution models (small cells): Use smaller pad (0.1-0.5m)\")\n",
    "# print(f\"â€¢ Regional models (large cells): Default pad=1.0 is usually fine\")\n",
    "# print(f\"â€¢ Very thin aquifers: May need smaller pad to allow streams in shallow layers\")\n",
    "# print(f\"â€¢ Uncertain elevations: Larger pad for safety margin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b514d312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Let's show a concrete example of why pad matters for model stability\n",
    "# print(\"=== CONCRETE EXAMPLE: WHY pad PREVENTS PROBLEMS ===\")\n",
    "\n",
    "# print(\"Scenario: Stream bottom at 19.659m, Layer 5 bottom at 18.913m\")\n",
    "# print(\"Distance between stream bottom and layer bottom: {:.3f}m\".format(19.659 - 18.913))\n",
    "\n",
    "# print(f\"\\nWithout pad (pad=0.0):\")\n",
    "# print(f\"â€¢ Stream bottom = 19.659m\")\n",
    "# print(f\"â€¢ Layer bottom = 18.913m\") \n",
    "# print(f\"â€¢ Available thickness for groundwater flow = 0.746m\")\n",
    "# print(f\"â€¢ âœ… Seems OK, but what if there are small elevation errors?\")\n",
    "\n",
    "# print(f\"\\nWith default pad=1.0:\")\n",
    "# print(f\"â€¢ Effective stream bottom = 19.659 - 1.0 = 18.659m\")\n",
    "# print(f\"â€¢ Gets assigned to Layer 7 (bottom = 18.319m)\")\n",
    "# print(f\"â€¢ Distance from stream to layer bottom = 18.659 - 18.319 = 0.340m\")\n",
    "# print(f\"â€¢ âœ… Much safer margin\")\n",
    "\n",
    "# print(f\"\\n=== REAL-WORLD ISSUES pad HELPS AVOID ===\")\n",
    "# print(f\"â€¢ DEM elevation errors (Â±0.1-1.0m typical)\")\n",
    "# print(f\"â€¢ Stream bathymetry uncertainties\")\n",
    "# print(f\"â€¢ Model discretization effects\")\n",
    "# print(f\"â€¢ Numerical precision in MODFLOW calculations\")\n",
    "# print(f\"â€¢ Convergence problems when hydraulic gradients become extreme\")\n",
    "\n",
    "# print(f\"\\n=== RECOMMENDATION FOR YOUR CASE ===\")\n",
    "# print(f\"Since you're working with strthick=0.1 (very thin), consider:\")\n",
    "# print(f\"â€¢ pad=0.1 to 0.5: Respects your thin streams while maintaining stability\")\n",
    "# print(f\"â€¢ pad=0.0: Only if you're confident in elevation accuracy\")\n",
    "# print(f\"â€¢ pad=1.0: Conservative default, but may push streams too deep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f284dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Let's check the dimensions and understand the coordinate system\n",
    "# print(\"BOTs shape:\", BOTs.shape)\n",
    "# print(\"TOPs shape:\", TOPs.shape)\n",
    "# print(\"DF_reach k,i,j ranges:\")\n",
    "# print(\"k range:\", DF_reach['k'].min(), \"to\", DF_reach['k'].max())  \n",
    "# print(\"i range:\", DF_reach['i'].min(), \"to\", DF_reach['i'].max())\n",
    "# print(\"j range:\", DF_reach['j'].min(), \"to\", DF_reach['j'].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e028684",
   "metadata": {},
   "source": [
    "## Investigating SFRmaker Source Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b08f2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Let's examine the SFRmaker source code to understand the add_observations method\n",
    "# import inspect\n",
    "\n",
    "# # Get the source code of the add_observations method\n",
    "# try:\n",
    "#     source_code = inspect.getsource(SFR_data.add_observations)\n",
    "#     print(\"SFRmaker add_observations method source code:\")\n",
    "#     print(\"=\" * 60)\n",
    "#     print(source_code)\n",
    "# except Exception as e:\n",
    "#     print(f\"Could not get source code: {e}\")\n",
    "    \n",
    "#     # Try to get the method signature at least\n",
    "#     try:\n",
    "#         sig = inspect.signature(SFR_data.add_observations)\n",
    "#         print(f\"\\nMethod signature: add_observations{sig}\")\n",
    "        \n",
    "#         # Get docstring\n",
    "#         doc = SFR_data.add_observations.__doc__\n",
    "#         if doc:\n",
    "#             print(f\"\\nDocstring:\\n{doc}\")\n",
    "#     except Exception as e2:\n",
    "#         print(f\"Could not get signature: {e2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f224cdc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Let's find the add_observations function that gets called\n",
    "# import sfrmaker\n",
    "# from sfrmaker import sfrdata\n",
    "\n",
    "# # Let's look for the standalone add_observations function\n",
    "# try:\n",
    "#     # Try to import the function directly\n",
    "#     from sfrmaker.sfrdata import add_observations as standalone_add_obs\n",
    "    \n",
    "#     source_code = inspect.getsource(standalone_add_obs)\n",
    "#     print(\"Standalone add_observations function source code:\")\n",
    "#     print(\"=\" * 70)\n",
    "#     print(source_code[:2000])  # Print first 2000 characters to see the logic\n",
    "#     print(\"...\" if len(source_code) > 2000 else \"\")\n",
    "    \n",
    "# except Exception as e:\n",
    "#     print(f\"Could not get standalone function: {e}\")\n",
    "    \n",
    "#     # Let's check the sfrmaker module structure\n",
    "#     print(\"\\nSFRmaker module contents:\")\n",
    "#     for attr in dir(sfrmaker):\n",
    "#         if 'obs' in attr.lower():\n",
    "#             print(f\"  {attr}\")\n",
    "            \n",
    "#     print(\"\\nSFRdata module contents:\")\n",
    "#     for attr in dir(sfrdata):\n",
    "#         if 'obs' in attr.lower():\n",
    "#             print(f\"  {attr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9560d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Let's get more of the source code, particularly the main logic\n",
    "# from sfrmaker.sfrdata import add_observations as standalone_add_obs\n",
    "\n",
    "# source_code = inspect.getsource(standalone_add_obs)\n",
    "# print(\"Full add_observations function source code (key parts):\")\n",
    "# print(\"=\" * 70)\n",
    "\n",
    "# # Find the main logic section\n",
    "# lines = source_code.split('\\n')\n",
    "# start_printing = False\n",
    "# for i, line in enumerate(lines):\n",
    "#     # Look for where the main processing starts\n",
    "#     if 'if data is None' in line or 'data = data.copy()' in line or 'for i, df in enumerate(data_list)' in line:\n",
    "#         start_printing = True\n",
    "        \n",
    "#     if start_printing:\n",
    "#         print(line)\n",
    "        \n",
    "#     # Stop if we get too long\n",
    "#     if start_printing and i > len(lines) - 20:  # Print most of the function\n",
    "#         break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
